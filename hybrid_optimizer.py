"""
èåˆä¼˜åŒ–ç­–ç•¥ï¼šç»“åˆ GP è´å¶æ–¯ä¼˜åŒ–å’Œ LLM æ¨è
ä½¿ç”¨ GP çš„é‡‡é›†å‡½æ•°ç»™ LLM æ¨èçš„ç‚¹æ‰“åˆ†ï¼Œåªæ¥å—"ä¸è¿‡åˆ†å·®"çš„ç‚¹
"""

from typing import List, Dict, Any, Optional, Tuple
import numpy as np
from ax_optimizer import BayesianOptimizer, ExperimentResult
from LLINBO_agent import (
    LLINBOAgent, ProblemContext, Parameter, PriorExperiment, LLMConfig
)


class HybridOptimizer:
    """
    èåˆä¼˜åŒ–å™¨ï¼šç»“åˆè´å¶æ–¯ä¼˜åŒ–ï¼ˆGPï¼‰å’Œå¤§æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¨èç­–ç•¥
    
    ç­–ç•¥ï¼š
    1. ä½¿ç”¨ LLM ç”Ÿæˆæ¨èç‚¹
    2. ä½¿ç”¨ GP çš„é‡‡é›†å‡½æ•°è¯„ä¼°è¿™äº›ç‚¹
    3. è·å– GP æ¨èçš„æœ€ä½³ç‚¹åŠå…¶é‡‡é›†å‡½æ•°å€¼
    4. å¯¹äºæ¯ä¸ª LLM ç‚¹ï¼Œå¦‚æœ (æœ€ä½³ç‚¹çš„é‡‡é›†å‡½æ•°å€¼ - LLMç‚¹çš„é‡‡é›†å‡½æ•°å€¼) < é˜ˆå€¼ï¼Œåˆ™æ¥å—
    """
    
    def __init__(
        self,
        # å‚æ•°ç©ºé—´å®šä¹‰ï¼ˆç”¨äº LLMï¼‰
        llm_parameters: List[Parameter],
        # å‚æ•°ç©ºé—´å®šä¹‰ï¼ˆç”¨äº GPï¼ŒAx æ ¼å¼ï¼‰
        gp_search_space: List[Dict[str, Any]],
        # ä¼˜åŒ–é…ç½®
        optimization_config: Dict[str, Any],
        # é—®é¢˜èƒŒæ™¯ï¼ˆç”¨äº LLMï¼‰
        problem_context: ProblemContext,
        # LLM é…ç½®
        llm_config: Optional[LLMConfig] = None,
        # GP ä¼˜åŒ–å™¨é…ç½®
        gp_experiment_name: str = "hybrid_optimization",
        gp_random_seed: Optional[int] = None,
        gp_surrogate_model_class: Optional[Any] = None,
        gp_kernel_class: Optional[Any] = None,
        gp_kernel_options: Optional[Dict[str, Any]] = None,
        gp_acquisition_function_class: Optional[Any] = None,
        gp_acquisition_function_options: Optional[Dict[str, Any]] = None,
        # èåˆç­–ç•¥å‚æ•°
        acquisition_threshold: float = 0.1,  # é‡‡é›†å‡½æ•°å·®å€¼é˜ˆå€¼ï¼ˆå›ºå®šé˜ˆå€¼ï¼Œå½“ use_dynamic_threshold=False æ—¶ä½¿ç”¨ï¼‰
        use_dynamic_threshold: bool = True,  # æ˜¯å¦ä½¿ç”¨åŸºäºæ–¹å·®çš„åŠ¨æ€é˜ˆå€¼
        threshold_multiplier: float = 1.0,  # åŠ¨æ€é˜ˆå€¼å€æ•°ï¼ˆé˜ˆå€¼ = threshold_multiplier * é¢„æµ‹æ ‡å‡†å·®ï¼‰
        random_seed: Optional[int] = None
    ):
        """
        åˆå§‹åŒ–èåˆä¼˜åŒ–å™¨
        
        Args:
            llm_parameters: LLM å‚æ•°ç©ºé—´å®šä¹‰ï¼ˆParameter åˆ—è¡¨ï¼‰
            gp_search_space: GP å‚æ•°ç©ºé—´å®šä¹‰ï¼ˆAx æ ¼å¼ï¼‰
            optimization_config: ä¼˜åŒ–é…ç½®
            problem_context: é—®é¢˜èƒŒæ™¯ï¼ˆç”¨äº LLMï¼‰
            llm_config: LLM é…ç½®
            gp_experiment_name: GP å®éªŒåç§°
            gp_random_seed: GP éšæœºç§å­
            gp_surrogate_model_class: GP ä»£ç†æ¨¡å‹ç±»
            gp_kernel_class: GP æ ¸å‡½æ•°ç±»
            gp_kernel_options: GP æ ¸å‡½æ•°é€‰é¡¹
            gp_acquisition_function_class: GP é‡‡é›†å‡½æ•°ç±»
            gp_acquisition_function_options: GP é‡‡é›†å‡½æ•°é€‰é¡¹
            acquisition_threshold: é‡‡é›†å‡½æ•°å·®å€¼é˜ˆå€¼ï¼ˆé»˜è®¤ 0.1ï¼Œå½“ use_dynamic_threshold=False æ—¶ä½¿ç”¨ï¼‰
            use_dynamic_threshold: æ˜¯å¦ä½¿ç”¨åŸºäºæ–¹å·®çš„åŠ¨æ€é˜ˆå€¼ï¼ˆé»˜è®¤ Trueï¼‰
            threshold_multiplier: åŠ¨æ€é˜ˆå€¼å€æ•°ï¼ˆé»˜è®¤ 1.0ï¼Œé˜ˆå€¼ = threshold_multiplier * é¢„æµ‹æ ‡å‡†å·®ï¼‰
            random_seed: éšæœºç§å­
        """
        self.acquisition_threshold = acquisition_threshold
        self.use_dynamic_threshold = use_dynamic_threshold
        self.threshold_multiplier = threshold_multiplier
        self.random_seed = random_seed
        
        # åˆå§‹åŒ– LLM Agent
        self.llm_agent = LLINBOAgent(
            problem_context=problem_context,
            parameters=llm_parameters,
            objectives=optimization_config.get("objectives", {}),
            llm_config=llm_config,
            prior_experiments=None,  # å…ˆéªŒæ•°æ®å°†é€šè¿‡ add_prior_experiments æ·»åŠ 
            random_seed=random_seed
        )
        
        # åˆå§‹åŒ– GP ä¼˜åŒ–å™¨
        self.gp_optimizer = BayesianOptimizer(
            search_space=gp_search_space,
            optimization_config=optimization_config,
            experiment_name=gp_experiment_name,
            random_seed=gp_random_seed,
            surrogate_model_class=gp_surrogate_model_class,
            kernel_class=gp_kernel_class,
            kernel_options=gp_kernel_options,
            acquisition_function_class=gp_acquisition_function_class,
            acquisition_function_options=gp_acquisition_function_options
        )
        
        # è®°å½•èåˆå†å²
        self.hybrid_history: List[Dict[str, Any]] = []
    
    def add_prior_experiments(self, experiments: List[ExperimentResult]) -> None:
        """
        æ·»åŠ å…ˆéªŒå®éªŒæ•°æ®åˆ° LLM å’Œ GP
        
        Args:
            experiments: å…ˆéªŒå®éªŒç»“æœåˆ—è¡¨
        """
        # æ·»åŠ åˆ° GP
        self.gp_optimizer.add_prior_experiments(experiments)
        
        # æ·»åŠ åˆ°LLMï¼ˆéœ€è¦æ£€æŸ¥é‡å¤ï¼Œé¿å…é‡å¤æ·»åŠ ï¼‰
        # è·å–ç°æœ‰å‚æ•°çš„å”¯ä¸€æ ‡è¯†
        existing_params = set()
        if self.llm_agent.prior_experiments:
            for exp in self.llm_agent.prior_experiments:
                param_key = tuple(sorted(exp.parameters.items()))
                existing_params.add(param_key)
        
        # åªæ·»åŠ ä¸é‡å¤çš„æ–°æ•°æ®
        for experiment in experiments:
            param_key = tuple(sorted(experiment.parameters.items()))
            if param_key not in existing_params:
                self.llm_agent.add_experiment_result(
                    parameters=experiment.parameters,
                    metrics=experiment.metrics,
                    metadata=experiment.metadata
                )
                existing_params.add(param_key)
    
    def _evaluate_acquisition_value(self, parameters: Dict[str, Any]) -> float:
        """
        è¯„ä¼°ç»™å®šå‚æ•°ç‚¹çš„é‡‡é›†å‡½æ•°å€¼
        
        ä½¿ç”¨ Ax çš„ TorchAdapter.evaluate_acquisition_function æ–¹æ³•ç²¾ç¡®è¯„ä¼°é‡‡é›†å‡½æ•°å€¼
        
        Args:
            parameters: å‚æ•°é…ç½®å­—å…¸
            
        Returns:
            acquisition_value: é‡‡é›†å‡½æ•°å€¼
        """
        try:
            from ax.core.observation import ObservationFeatures
            
            # åˆ›å»ºè§‚å¯Ÿç‰¹å¾
            obsf = ObservationFeatures(parameters=parameters)
            
            # è·å– GenerationStrategy çš„ adapter
            gs = self.gp_optimizer.ax_client.generation_strategy
            adapter = gs.adapter
            
            if adapter is None:
                # å¦‚æœè¿˜æ²¡æœ‰ adapterï¼Œè¿”å›ä¸€ä¸ªé»˜è®¤å€¼
                print("âš ï¸ è­¦å‘Š: GenerationStrategy è¿˜æ²¡æœ‰ adapterï¼Œæ— æ³•è¯„ä¼°é‡‡é›†å‡½æ•°å€¼")
                return 0.0
            
            # ä½¿ç”¨ TorchAdapter çš„ evaluate_acquisition_function æ–¹æ³•
            # è¿™æ˜¯ Ax æä¾›çš„ç²¾ç¡®è¯„ä¼°é‡‡é›†å‡½æ•°å€¼çš„ API
            try:
                acqf_values = adapter.evaluate_acquisition_function(
                    observation_features=[obsf],
                    search_space=self.gp_optimizer.ax_client.experiment.search_space,
                    optimization_config=self.gp_optimizer.ax_client.experiment.optimization_config,
                    pending_observations=None,
                    fixed_features=None,
                    acq_options=None
                )
                
                if acqf_values and len(acqf_values) > 0:
                    return float(acqf_values[0])
                else:
                    return 0.0
                    
            except Exception as e:
                print(f"âš ï¸ ä½¿ç”¨ TorchAdapter.evaluate_acquisition_function å¤±è´¥: {e}")
                # å¦‚æœå¤±è´¥ï¼Œè¿”å›é»˜è®¤å€¼
                return 0.0
                
        except Exception as e:
            print(f"âš ï¸ è¯„ä¼°é‡‡é›†å‡½æ•°å€¼æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return 0.0
    
    def _get_prediction_std(self, parameters: Dict[str, Any]) -> float:
        """
        è·å–ç»™å®šå‚æ•°ç‚¹çš„é¢„æµ‹æ ‡å‡†å·®ï¼ˆå½’ä¸€åŒ–åçš„å¹³å‡å˜å¼‚ç³»æ•°ï¼‰
        
        ä½¿ç”¨ Ax çš„ get_model_predictions æ–¹æ³•è·å–é«˜æ–¯è¿‡ç¨‹çš„é¢„æµ‹æ ‡å‡†å·®
        å¯¹äºå¤šç›®æ ‡ä¼˜åŒ–ï¼Œä½¿ç”¨å˜å¼‚ç³»æ•°ï¼ˆCV = std/meanï¼‰æ¥æ¶ˆé™¤é‡çº²å½±å“
        
        Args:
            parameters: å‚æ•°é…ç½®å­—å…¸
            
        Returns:
            prediction_std: é¢„æµ‹æ ‡å‡†å·®ï¼ˆå¦‚æœå¤šç›®æ ‡ï¼Œè¿”å›å¹³å‡å˜å¼‚ç³»æ•°ï¼Œæ— é‡çº²ï¼‰
        """
        try:
            # ä½¿ç”¨ Ax çš„ get_model_predictions æ–¹æ³•è·å–é¢„æµ‹
            predictions_dict = self.gp_optimizer.ax_client.get_model_predictions(
                metric_names=None,  # è·å–æ‰€æœ‰æŒ‡æ ‡
                parameterizations={0: parameters}
            )
            
            if not predictions_dict or 0 not in predictions_dict:
                return 0.0
            
            # è·å–æ‰€æœ‰æŒ‡æ ‡çš„é¢„æµ‹
            metric_predictions = predictions_dict[0]
            
            # è®¡ç®—å˜å¼‚ç³»æ•°ï¼ˆCV = std/meanï¼‰ï¼Œç„¶åå¹³å‡
            # å˜å¼‚ç³»æ•°æ˜¯æ— é‡çº²çš„ï¼Œå¯ä»¥ç”¨äºä¸åŒé‡çº²ç›®æ ‡çš„æ¯”è¾ƒ
            cv_values = []
            for metric_name, (mean, sem) in metric_predictions.items():
                if abs(mean) > 1e-10:  # é¿å…é™¤é›¶
                    cv = abs(sem / mean)  # å˜å¼‚ç³»æ•°ï¼ˆæ— é‡çº²ï¼‰
                    cv_values.append(cv)
                else:
                    # å¦‚æœå‡å€¼æ¥è¿‘0ï¼Œä½¿ç”¨ç»å¯¹æ ‡å‡†å·®
                    cv_values.append(abs(sem))
            
            if cv_values:
                # è¿”å›å¹³å‡å˜å¼‚ç³»æ•°
                avg_cv = sum(cv_values) / len(cv_values)
                return avg_cv
            else:
                return 0.0
                
        except Exception as e:
            print(f"âš ï¸ è·å–é¢„æµ‹æ ‡å‡†å·®å¤±è´¥: {e}")
            return 0.0
    
    def _get_gp_best_acquisition_value(self) -> Tuple[Dict[str, Any], float, float]:
        """
        è·å– GP æ¨èçš„æœ€ä½³ç‚¹åŠå…¶é‡‡é›†å‡½æ•°å€¼å’Œé¢„æµ‹æ ‡å‡†å·®
        
        Returns:
            best_parameters: æœ€ä½³å‚æ•°é…ç½®
            best_acquisition_value: æœ€ä½³é‡‡é›†å‡½æ•°å€¼
            best_prediction_std: æœ€ä½³ç‚¹çš„é¢„æµ‹æ ‡å‡†å·®
        """
        try:
            # è·å– GP æ¨èçš„æœ€ä½³ç‚¹
            gp_trials, _ = self.gp_optimizer.ax_client.get_next_trials(max_trials=1)
            
            if not gp_trials:
                return {}, 0.0, 0.0
            
            # è·å–ç¬¬ä¸€ä¸ªï¼ˆæœ€ä½³ï¼‰æ¨èç‚¹
            best_trial_index = list(gp_trials.keys())[0]
            best_parameters = gp_trials[best_trial_index]
            
            # è¯„ä¼°è¯¥ç‚¹çš„é‡‡é›†å‡½æ•°å€¼
            best_acquisition_value = self._evaluate_acquisition_value(best_parameters)
            
            # è·å–è¯¥ç‚¹çš„é¢„æµ‹æ ‡å‡†å·®
            best_prediction_std = self._get_prediction_std(best_parameters)
            
            return best_parameters, best_acquisition_value, best_prediction_std
            
        except Exception as e:
            print(f"âš ï¸ è·å– GP æœ€ä½³ç‚¹å¤±è´¥: {e}")
            return {}, 0.0, 0.0
    
    def suggest_parameters(
        self,
        num_suggestions: int = 1,
        use_llm: bool = True,
        use_gp: bool = True,
        print_details: bool = False
    ) -> List[Dict[str, Any]]:
        """
        ç”Ÿæˆèåˆæ¨èå‚æ•°
        
        Args:
            num_suggestions: éœ€è¦ç”Ÿæˆçš„å»ºè®®æ•°é‡
            use_llm: æ˜¯å¦ä½¿ç”¨ LLM æ¨è
            use_gp: æ˜¯å¦ä½¿ç”¨ GP æ¨è
            print_details: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
            
        Returns:
            accepted_suggestions: è¢«æ¥å—çš„æ¨èå‚æ•°åˆ—è¡¨
        """
        accepted_suggestions = []
        
        # 1. è·å– GP æ¨èçš„æœ€ä½³ç‚¹åŠå…¶é‡‡é›†å‡½æ•°å€¼å’Œé¢„æµ‹æ ‡å‡†å·®
        gp_best_params, gp_best_acq_value, gp_best_std = self._get_gp_best_acquisition_value()
        
        if print_details:
            print(f"\nğŸ“Š GP æ¨èæœ€ä½³ç‚¹:")
            print(f"   å‚æ•°: {gp_best_params}")
            print(f"   é‡‡é›†å‡½æ•°å€¼: {gp_best_acq_value:.6f}")
            print(f"   é¢„æµ‹æ ‡å‡†å·®: {gp_best_std:.6f}")
        
        # 2. ä½¿ç”¨ LLM ç”Ÿæˆæ¨èç‚¹
        llm_suggestions = []
        if use_llm:
            try:
                llm_suggestions = self.llm_agent.suggest_parameters(
                    num_suggestions=num_suggestions * 2,  # ç”Ÿæˆæ›´å¤šå€™é€‰ç‚¹
                    print_prompt=True,
                    print_response=True
                )
                if print_details:
                    print(f"\nğŸ¤– LLM ç”Ÿæˆäº† {len(llm_suggestions)} ä¸ªæ¨èç‚¹")
            except Exception as e:
                print(f"âš ï¸ LLM æ¨èå¤±è´¥: {e}")
        
        # 3. ä½¿ç”¨ GP é‡‡é›†å‡½æ•°è¯„ä¼° LLM æ¨èçš„ç‚¹
        evaluated_llm_suggestions = []
        for llm_params in llm_suggestions:
            try:
                acq_value = self._evaluate_acquisition_value(llm_params)
                
                # è·å–è¯¥ç‚¹çš„é¢„æµ‹æ ‡å‡†å·®
                prediction_std = self._get_prediction_std(llm_params)
                
                # è®¡ç®—ä¸æœ€ä½³ç‚¹çš„å·®å€¼
                acq_diff = gp_best_acq_value - acq_value
                
                # è®¡ç®—åŠ¨æ€é˜ˆå€¼ï¼ˆåŸºäºè¯¥ç‚¹çš„é¢„æµ‹æ ‡å‡†å·®ï¼‰
                if self.use_dynamic_threshold:
                    # ä½¿ç”¨è¯¥ç‚¹çš„é¢„æµ‹æ ‡å‡†å·®ä½œä¸ºé˜ˆå€¼
                    dynamic_threshold = self.threshold_multiplier * prediction_std
                    # å¦‚æœæ ‡å‡†å·®ä¸º0æˆ–å¤ªå°ï¼Œä½¿ç”¨å›ºå®šé˜ˆå€¼ä½œä¸ºä¸‹é™
                    if dynamic_threshold < self.acquisition_threshold:
                        dynamic_threshold = self.acquisition_threshold
                else:
                    # ä½¿ç”¨å›ºå®šé˜ˆå€¼
                    dynamic_threshold = self.acquisition_threshold
                
                evaluated_llm_suggestions.append({
                    "parameters": llm_params,
                    "acquisition_value": acq_value,
                    "acquisition_diff": acq_diff,
                    "prediction_std": prediction_std,
                    "threshold": dynamic_threshold,
                    "source": "LLM"
                })
                
                if print_details:
                    print(f"\n   LLM æ¨èç‚¹: {llm_params}")
                    print(f"   é‡‡é›†å‡½æ•°å€¼: {acq_value:.6f}")
                    print(f"   é¢„æµ‹æ ‡å‡†å·®: {prediction_std:.6f}")
                    print(f"   ä¸æœ€ä½³ç‚¹å·®å€¼: {acq_diff:.6f}")
                    print(f"   åŠ¨æ€é˜ˆå€¼: {dynamic_threshold:.6f} ({'åŸºäºæ–¹å·®' if self.use_dynamic_threshold else 'å›ºå®š'})")
                    print(f"   æ˜¯å¦æ¥å—: {'âœ…' if acq_diff < dynamic_threshold else 'âŒ'}")
                    
            except Exception as e:
                print(f"âš ï¸ è¯„ä¼° LLM æ¨èç‚¹å¤±è´¥: {e}")
        
        # 4. ç­›é€‰"ä¸è¿‡åˆ†å·®"çš„ LLM æ¨èç‚¹ï¼ˆä½¿ç”¨åŠ¨æ€é˜ˆå€¼ï¼‰
        # ç‰¹æ®Šæƒ…å†µï¼šå¦‚æœ GP æœ€ä½³ç‚¹çš„é‡‡é›†å‡½æ•°å€¼ä¸ºè´Ÿï¼Œè¯´æ˜ GP å¯¹å½“å‰æœç´¢ç©ºé—´ä¿¡å¿ƒä¸è¶³
        # æ­¤æ—¶åº”è¯¥æ›´ä¿¡ä»» LLM çš„æ¨èï¼Œç›´æ¥é‡‡çº³å‰ n ä¸ª LLM æ¨èçš„ç‚¹
        if gp_best_acq_value < -0.5:
            if print_details:
                print(f"\nâš ï¸ GP æœ€ä½³ç‚¹çš„é‡‡é›†å‡½æ•°å€¼ä¸ºè´Ÿ ({gp_best_acq_value:.6f})ï¼Œè¯´æ˜ GP å¯¹å½“å‰æœç´¢ç©ºé—´ä¿¡å¿ƒä¸è¶³")
                print(f"   é‡‡ç”¨ç‰¹æ®Šç­–ç•¥ï¼šç›´æ¥é‡‡çº³å‰ {num_suggestions} ä¸ª LLM æ¨èç‚¹ï¼ˆæŒ‰å¤§æ¨¡å‹æ¨èä¿¡å¿ƒä»é«˜åˆ°ä½æ’åºï¼‰")
            
            # ç›´æ¥æ¨èå‰num_suggestionsä¸ªLLMæ¨èçš„ç‚¹
            valid_suggestions = evaluated_llm_suggestions[:num_suggestions]
            
        else:
            # æ­£å¸¸æƒ…å†µï¼šä½¿ç”¨é˜ˆå€¼ç­›é€‰
            # å…ˆç­›é€‰å‡ºæ‰€æœ‰æ»¡è¶³é˜ˆå€¼çš„ç‚¹
            valid_suggestions = []
            for llm_suggestion in evaluated_llm_suggestions:
                if llm_suggestion["acquisition_diff"] < llm_suggestion["threshold"]:
                    valid_suggestions.append(llm_suggestion)
            
            # å¦‚æœæ»¡è¶³é˜ˆå€¼çš„ç‚¹è¶…è¿‡ num_suggestions ä¸ªï¼ŒæŒ‰é‡‡é›†å‡½æ•°å·®å€¼æ’åºï¼Œä¼˜å…ˆé€‰æ‹©å·®å€¼æ›´å°çš„ç‚¹
            if len(valid_suggestions) > num_suggestions:
                # æŒ‰é‡‡é›†å‡½æ•°å·®å€¼ä»å°åˆ°å¤§æ’åºï¼ˆå·®å€¼è¶Šå°ï¼Œè¯´æ˜è¶Šæ¥è¿‘æœ€ä½³ç‚¹ï¼‰
                valid_suggestions.sort(key=lambda x: x["acquisition_diff"])
                if print_details:
                    print(f"\nğŸ“Š æ»¡è¶³é˜ˆå€¼çš„ç‚¹æœ‰ {len(valid_suggestions)} ä¸ªï¼ŒæŒ‰é‡‡é›†å‡½æ•°å·®å€¼æ’åºåé€‰æ‹©å‰ {num_suggestions} ä¸ª")
                    print(f"   æ’åºåçš„å‰ {num_suggestions} ä¸ªç‚¹çš„é‡‡é›†å‡½æ•°å·®å€¼:")
                    for i, suggestion in enumerate(valid_suggestions[:num_suggestions], 1):
                        print(f"   {i}. å·®å€¼: {suggestion['acquisition_diff']:.6f}, é˜ˆå€¼: {suggestion['threshold']:.6f}")
                # åªä¿ç•™å‰ num_suggestions ä¸ª
                valid_suggestions = valid_suggestions[:num_suggestions]
        
        # å°†ç­›é€‰åçš„ç‚¹æ·»åŠ åˆ°æ¥å—åˆ—è¡¨
        for suggestion in valid_suggestions:
            accepted_suggestions.append(suggestion["parameters"])
        
        # 5. å¦‚æœè¿˜éœ€è¦æ›´å¤šæ¨èï¼Œä½¿ç”¨ GP æ¨èè¡¥å……
        if use_gp and len(accepted_suggestions) < num_suggestions:
            try:
                gp_trials = self.gp_optimizer.get_next_parameters(
                    n=num_suggestions - len(accepted_suggestions)
                )
                for trial_index, gp_params in gp_trials:
                    # æ£€æŸ¥æ˜¯å¦ä¸å·²æ¥å—çš„ LLM æ¨èé‡å¤
                    is_duplicate = False
                    for accepted in accepted_suggestions:
                        if self._parameters_equal(accepted, gp_params):
                            is_duplicate = True
                            break
                    
                    if not is_duplicate:
                        accepted_suggestions.append(gp_params)
                        if print_details:
                            print(f"\n   GP è¡¥å……æ¨èç‚¹: {gp_params}")
                            
            except Exception as e:
                print(f"âš ï¸ GP æ¨èå¤±è´¥: {e}")
        
        # 6. è®°å½•èåˆå†å²
        self.hybrid_history.append({
            "gp_best_params": gp_best_params,
            "gp_best_acq_value": gp_best_acq_value,
            "gp_best_std": gp_best_std,
            "llm_suggestions_count": len(llm_suggestions),
            "accepted_count": len(accepted_suggestions),
            "evaluated_llm_suggestions": evaluated_llm_suggestions,
            "use_dynamic_threshold": self.use_dynamic_threshold
        })
        
        if print_details:
            print(f"\nâœ… èåˆæ¨èå®Œæˆ:")
            print(f"   LLM æ¨èæ•°: {len(llm_suggestions)}")
            print(f"   æ¥å—æ•°: {len(accepted_suggestions)}")
            print(f"   æœ€ç»ˆæ¨èæ•°: {len(accepted_suggestions)}")
        
        return accepted_suggestions[:num_suggestions]
    
    def _parameters_equal(self, params1: Dict[str, Any], params2: Dict[str, Any]) -> bool:
        """æ£€æŸ¥ä¸¤ä¸ªå‚æ•°é…ç½®æ˜¯å¦ç›¸ç­‰"""
        if set(params1.keys()) != set(params2.keys()):
            return False
        
        for key in params1.keys():
            val1 = params1[key]
            val2 = params2[key]
            
            # å¯¹äºæµ®ç‚¹æ•°ï¼Œä½¿ç”¨å®¹å·®æ¯”è¾ƒ
            if isinstance(val1, float) or isinstance(val2, float):
                if abs(float(val1) - float(val2)) > 1e-6:
                    return False
            else:
                if val1 != val2:
                    return False
        
        return True
    
    def update_experiment(
        self,
        parameters: Dict[str, Any],
        metrics: Dict[str, float],
        metadata: Optional[Dict[str, Any]] = None
    ) -> None:
        """
        æ›´æ–°å®éªŒç»“æœ
        
        Args:
            parameters: å‚æ•°é…ç½®
            metrics: æŒ‡æ ‡ç»“æœ
            metadata: é¢å¤–å…ƒæ•°æ®
        """
        # æ›´æ–°åˆ° GP
        # éœ€è¦å…ˆæ‰¾åˆ°å¯¹åº”çš„ trial_index
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼šåˆ›å»ºä¸€ä¸ªæ–°çš„ trial
        trial_index, _ = self.gp_optimizer.ax_client.attach_trial(parameters)
        self.gp_optimizer.update_experiment(trial_index, metrics, metadata)
        
        # æ›´æ–°åˆ° LLM
        self.llm_agent.add_experiment_result(parameters, metrics, metadata)
    
    def get_best_parameters(self) -> Tuple[Dict[str, Any], Dict[str, float]]:
        """
        è·å–å½“å‰æœ€ä¼˜å‚æ•°é…ç½®
        
        Returns:
            best_parameters: æœ€ä¼˜å‚æ•°é…ç½®
            best_metrics: æœ€ä¼˜æŒ‡æ ‡å€¼
        """
        return self.gp_optimizer.get_best_parameters()
    
    def get_optimization_history(self):
        """è·å–ä¼˜åŒ–å†å²è®°å½•"""
        return self.gp_optimizer.get_optimization_history()


def example_usage():
    """ä½¿ç”¨ç¤ºä¾‹"""
    from LLINBO_agent import ProblemContext, Parameter, LLMConfig
    import os
    
    # 1. å®šä¹‰é—®é¢˜èƒŒæ™¯
    problem_context = ProblemContext(
        problem_description="ä¼˜åŒ–æ¿€å…‰åˆ‡å‰²å·¥è‰ºå‚æ•°ï¼Œä»¥æé«˜åˆ‡å‰²è´¨é‡å’Œæ•ˆç‡",
        industry="åˆ¶é€ ä¸š - æ¿€å…‰åŠ å·¥",
        domain_knowledge="æ¿€å…‰åŠŸç‡ã€åˆ‡å‰²é€Ÿåº¦å’Œé¢‘ç‡å¯¹è¡¨é¢ç²—ç³™åº¦å’Œåˆ‡ç¼å®½åº¦æœ‰æ˜¾è‘—å½±å“",
        constraints=["åŠŸç‡ä¸èƒ½è¶…è¿‡è®¾å¤‡ä¸Šé™", "é€Ÿåº¦å¿…é¡»ä¿è¯åˆ‡å‰²è´¨é‡"],
        optimization_goals=["æœ€å°åŒ–è¡¨é¢ç²—ç³™åº¦", "æœ€å°åŒ–åˆ‡ç¼å®½åº¦"]
    )
    
    # 2. å®šä¹‰ LLM å‚æ•°ç©ºé—´
    llm_parameters = [
        Parameter(
            name="power",
            type="range",
            bounds=[1000, 3000],
            value_type="int",
            description="æ¿€å…‰åŠŸç‡",
            unit="W"
        ),
        Parameter(
            name="speed",
            type="range",
            bounds=[10.0, 50.0],
            value_type="float",
            description="åˆ‡å‰²é€Ÿåº¦",
            unit="mm/s"
        ),
        Parameter(
            name="frequency",
            type="choice",
            values=[500, 1000, 1500, 2000],
            value_type="int",
            description="è„‰å†²é¢‘ç‡",
            unit="Hz"
        )
    ]
    
    # 3. å®šä¹‰ GP å‚æ•°ç©ºé—´ï¼ˆAx æ ¼å¼ï¼‰
    gp_search_space = [
        {
            "name": "power",
            "type": "range",
            "bounds": [1000, 3000],
            "value_type": "int"
        },
        {
            "name": "speed",
            "type": "range",
            "bounds": [10, 50],
            "value_type": "float"
        },
        {
            "name": "frequency",
            "type": "choice",
            "values": [500, 1000, 1500, 2000],
            "value_type": "int"
        }
    ]
    
    # 4. å®šä¹‰ä¼˜åŒ–é…ç½®
    optimization_config = {
        "objectives": {
            "roughness": {"minimize": True},
            "kerf_width": {"minimize": True}
        }
    }
    
    # 5. åˆ›å»ºèåˆä¼˜åŒ–å™¨
    llm_config = LLMConfig(
        model_name="gpt-4o-mini",
        api_key=os.getenv("OPENAI_API_KEY"),
        base_url="https://api.openai.com/v1"
    )
    
    hybrid_optimizer = HybridOptimizer(
        llm_parameters=llm_parameters,
        gp_search_space=gp_search_space,
        optimization_config=optimization_config,
        problem_context=problem_context,
        llm_config=llm_config,
        acquisition_threshold=0.1,  # å›ºå®šé˜ˆå€¼ï¼ˆå½“ use_dynamic_threshold=False æ—¶ä½¿ç”¨ï¼‰
        use_dynamic_threshold=True,  # ä½¿ç”¨åŸºäºæ–¹å·®çš„åŠ¨æ€é˜ˆå€¼
        threshold_multiplier=1.0,  # åŠ¨æ€é˜ˆå€¼å€æ•°ï¼ˆé˜ˆå€¼ = threshold_multiplier * é¢„æµ‹æ ‡å‡†å·®ï¼‰
        random_seed=42
    )
    
    # 6. æ·»åŠ å…ˆéªŒæ•°æ®
    prior_experiments = [
        ExperimentResult(
            parameters={"power": 2000, "speed": 30.0, "frequency": 1000},
            metrics={"roughness": 2.5, "kerf_width": 0.15}
        ),
        ExperimentResult(
            parameters={"power": 2500, "speed": 40.0, "frequency": 1500},
            metrics={"roughness": 1.8, "kerf_width": 0.18}
        )
    ]
    hybrid_optimizer.add_prior_experiments(prior_experiments)
    
    # 7. ç”Ÿæˆèåˆæ¨è
    suggestions = hybrid_optimizer.suggest_parameters(
        num_suggestions=3,
        use_llm=True,
        use_gp=True,
        print_details=True
    )
    
    print("\nğŸ“Š æœ€ç»ˆæ¥å—çš„æ¨è:")
    for i, suggestion in enumerate(suggestions, 1):
        print(f"\næ¨è {i}:")
        for param_name, param_value in suggestion.items():
            print(f"  {param_name}: {param_value}")


if __name__ == "__main__":
    example_usage()

