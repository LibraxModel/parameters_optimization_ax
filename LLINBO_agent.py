"""
LLINBO Agent: Large Language Model for Bayesian Optimization Agent
åŸºäºå¤§æ¨¡å‹çš„è´å¶æ–¯ä¼˜åŒ–æ™ºèƒ½ä½“

åŠŸèƒ½ï¼š
1. æ¥å—å¯é…ç½®çš„ä¼˜åŒ–é—®é¢˜èƒŒæ™¯å’Œè¡Œä¸šæè¿°
2. æ”¯æŒå¯é…ç½®çš„å‚æ•°ç©ºé—´èŒƒå›´
3. åˆ©ç”¨å†å²å…ˆéªŒå®éªŒæ•°æ®è¿›è¡Œä¼˜åŒ–å»ºè®®
4. ä½¿ç”¨å¤§æ¨¡å‹æ¨¡æ‹Ÿè´å¶æ–¯ä¼˜åŒ–è¿‡ç¨‹
"""

from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, field
import json
import os
import pandas as pd
import numpy as np
from abc import ABC, abstractmethod


@dataclass
class ProblemContext:
    """ä¼˜åŒ–é—®é¢˜èƒŒæ™¯é…ç½®"""
    problem_description: str  # é—®é¢˜æè¿°
    industry: str  # è¡Œä¸šé¢†åŸŸ
    domain_knowledge: Optional[str] = None  # é¢†åŸŸçŸ¥è¯†
    constraints: Optional[List[str]] = None  # çº¦æŸæ¡ä»¶
    optimization_goals: Optional[List[str]] = None  # ä¼˜åŒ–ç›®æ ‡è¯´æ˜


@dataclass
class Parameter:
    """å‚æ•°ç©ºé—´å®šä¹‰"""
    name: str  # å‚æ•°åç§°
    type: str  # å‚æ•°ç±»å‹: "range" æˆ– "choice"
    bounds: Optional[List[float]] = None  # èŒƒå›´å‚æ•°: [min, max]
    values: Optional[List[Any]] = None  # é€‰æ‹©å‚æ•°: [value1, value2, ...]
    value_type: str = "float"  # å€¼ç±»å‹: "int", "float", "str"
    description: Optional[str] = None  # å‚æ•°æè¿°
    unit: Optional[str] = None  # å•ä½   


@dataclass
class PriorExperiment:
    """å…ˆéªŒå®éªŒæ•°æ®"""
    parameters: Dict[str, Any]  # å‚æ•°é…ç½®
    metrics: Dict[str, float]  # å®éªŒç»“æœæŒ‡æ ‡
    metadata: Optional[Dict[str, Any]] = None  # é¢å¤–å…ƒæ•°æ®


@dataclass
class LLMConfig:
    """å¤§æ¨¡å‹é…ç½®"""
    model_name: str = "gpt-4"  # æ¨¡å‹åç§°
    api_key: Optional[str] = None  # APIå¯†é’¥
    base_url: Optional[str] = None  # APIåŸºç¡€URL
    
    


class LLMProvider(ABC):
    """å¤§æ¨¡å‹æä¾›è€…æŠ½è±¡åŸºç±»"""
    
    @abstractmethod
    def generate(self, prompt: str, **kwargs) -> str:
        """ç”Ÿæˆæ–‡æœ¬"""
        pass


class OpenAIProvider(LLMProvider):
    """OpenAI API æä¾›è€…"""
    
    def __init__(self, config: LLMConfig):
        self.config = config
        try:
            import openai
            self.client = openai.OpenAI(
                api_key=config.api_key or None,
                base_url=config.base_url
            )
        except ImportError:
            raise ImportError("è¯·å®‰è£… openai åº“: pip install openai")
        except Exception as e:
            raise RuntimeError(f"åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯å¤±è´¥: {e}")
    
    def generate(self, prompt: str, **kwargs) -> str:
        """ä½¿ç”¨ OpenAI API ç”Ÿæˆæ–‡æœ¬"""
        try:
            response = self.client.chat.completions.create(
                model=self.config.model_name,
                messages=[
                    {"role": "system", "content": """
                    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å‚æ•°ä¼˜åŒ–ç®—æ³•ä¸“å®¶ï¼Œ
                    æ“…é•¿åŸºäºç”¨æˆ·æä¾›çš„å…ˆéªŒå®éªŒæ•°æ®å’Œä¸“ä¸šé¢†åŸŸä»ä½ çš„è®­ç»ƒæ•°æ®ä¸­è·å–èƒŒæ™¯çŸ¥è¯†è¿›è¡Œè¯¥é¢†åŸŸçš„å‚æ•°ä¼˜åŒ–æ¨èã€‚
                    å¦‚æœä½ æœ‰80%çš„æŠŠæ¡ä½ æ‰€æ¨èçš„å‚æ•°ç»„åˆå¾—åˆ°çš„å®éªŒç»“æœä¼šæ¯”å…ˆéªŒæ•°æ®ä¸­çš„æœ€å¥½çš„ç»“æœæ›´å¥½ï¼Œæ‰æ¨èè¿™ä¸ªå‚æ•°ç»„åˆã€‚
                    è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¿”å›ç»“æœï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–å†…å®¹ä¸”ä¸å¿…è¾“å‡ºæ¨èåŸå› ã€‚
                    âš ï¸è­¦å‘Šï¼ä½ çš„æ¨èä¸èƒ½æ¥è‡ªäºå…ˆéªŒæ•°æ®ã€‚
                    
                    """ },
                    {"role": "user", "content": prompt}
                ]
            
            
        )
        except Exception as e:
            raise RuntimeError(f"è°ƒç”¨ OpenAI API å¤±è´¥: {e}")
        return response.choices[0].message.content


class LLINBOAgent:
    """
    åŸºäºå¤§æ¨¡å‹çš„è´å¶æ–¯ä¼˜åŒ–æ™ºèƒ½ä½“
    
    ä½¿ç”¨å¤§æ¨¡å‹æ¥æ¨¡æ‹Ÿè´å¶æ–¯ä¼˜åŒ–è¿‡ç¨‹ï¼Œç»“åˆé—®é¢˜èƒŒæ™¯ã€å‚æ•°ç©ºé—´å’Œå…ˆéªŒæ•°æ®
    ç”Ÿæˆä¼˜åŒ–å»ºè®®ã€‚
    """
    
    def __init__(
        self,
        problem_context: ProblemContext,
        parameters: List[Parameter],
        objectives: Dict[str, Dict[str, bool]],  # {"metric_name": {"minimize": bool}}
        llm_config: Optional[LLMConfig] = None,
        prior_experiments: Optional[List[PriorExperiment]] = None,
        random_seed: Optional[int] = None
    ):
        """
        åˆå§‹åŒ– LLINBO Agent
        
        Args:
            problem_context: ä¼˜åŒ–é—®é¢˜èƒŒæ™¯
            parameters: å‚æ•°å®šä¹‰åˆ—è¡¨
            objectives: ä¼˜åŒ–ç›®æ ‡é…ç½®
            llm_config: å¤§æ¨¡å‹é…ç½®ï¼ˆå¯é€‰ï¼‰
            prior_experiments: å…ˆéªŒå®éªŒæ•°æ®ï¼ˆå¯é€‰ï¼‰
            random_seed: éšæœºç§å­ï¼ˆå¯é€‰ï¼‰
        """
        self.problem_context = problem_context
        self.parameters = parameters
        self.objectives = objectives
        self.prior_experiments = prior_experiments or []
        self.random_seed = random_seed
        
        # åˆå§‹åŒ–å¤§æ¨¡å‹æä¾›è€…
        if llm_config is None:
            llm_config = LLMConfig()
        self.llm_config = llm_config
        
        # æ ¹æ®é…ç½®é€‰æ‹©æä¾›è€…
        self.llm_provider = OpenAIProvider(llm_config)

        # ä¼˜åŒ–å†å²è®°å½•

        self.optimization_history: List[Dict[str, Any]] = []
    
    def _build_context_prompt(self) -> str:
        """æ„å»ºåŒ…å«é—®é¢˜èƒŒæ™¯çš„æç¤ºè¯"""
        prompt_parts = [
            "# ä¼˜åŒ–é—®é¢˜èƒŒæ™¯",
            f"**é—®é¢˜æè¿°**: {self.problem_context.problem_description}",
            f"**è¡Œä¸šé¢†åŸŸ**: {self.problem_context.industry}",
        ]
        
        if self.problem_context.domain_knowledge:
            prompt_parts.append(f"**é¢†åŸŸçŸ¥è¯†**: {self.problem_context.domain_knowledge}")
        
        if self.problem_context.constraints:
            prompt_parts.append(f"**çº¦æŸæ¡ä»¶**: {', '.join(self.problem_context.constraints)}")
        
        if self.problem_context.optimization_goals:
            prompt_parts.append(f"**ä¼˜åŒ–ç›®æ ‡**: {', '.join(self.problem_context.optimization_goals)}")
        
        return "\n".join(prompt_parts)
    
    def _build_parameter_space_prompt(self) -> str:
        """æ„å»ºå‚æ•°ç©ºé—´æè¿°æç¤ºè¯"""
        prompt_parts = [
            "# å‚æ•°ç©ºé—´å®šä¹‰",
            "ä»¥ä¸‹æ˜¯éœ€è¦ä¼˜åŒ–çš„å‚æ•°åŠå…¶èŒƒå›´ï¼š",
            "",
            "**é‡è¦æç¤º**ï¼šå¦‚æœå‚æ•°æ˜¯ç¦»æ•£å‚æ•°ï¼Œå¿…é¡»ä»åˆ—å‡ºçš„å¯é€‰å€¼ä¸­é€‰æ‹©ï¼Œä¸èƒ½é€‰æ‹©å…¶ä»–å€¼ï¼›å¦‚æœå‚æ•°æ˜¯è¿ç»­å‚æ•°ï¼Œå€¼å¿…é¡»åœ¨ [æœ€å°å€¼, æœ€å¤§å€¼] èŒƒå›´å†…ã€‚"
        ]
        
        for i, param in enumerate(self.parameters, 1):
            param_desc = [f"{i}. **{param.name}**"]
            
            if param.description:
                param_desc.append(f"   æè¿°: {param.description}")
            
            if param.type == "range" and param.bounds:
                param_desc.append(f"   ç±»å‹: è¿ç»­å‚æ•°")
                param_desc.append(f"   èŒƒå›´: [{param.bounds[0]}, {param.bounds[1]}]")
                if param.unit:
                    param_desc.append(f"   å•ä½: {param.unit}")
            elif param.type == "choice" and param.values:
                param_desc.append(f"   ç±»å‹: ç¦»æ•£å‚æ•°ï¼ˆå¿…é¡»ä»ä»¥ä¸‹å€¼ä¸­é€‰æ‹©ä¸€ä¸ªï¼‰")
                # æ˜ç¡®åˆ—å‡ºæ‰€æœ‰å¯é€‰å€¼
                values_str = ", ".join([str(v) for v in param.values])
                param_desc.append(f"   å¯é€‰å€¼åˆ—è¡¨: [{values_str}]")
                param_desc.append(f"   å¯é€‰å€¼æ•°é‡: {len(param.values)} ä¸ª")
                param_desc.append(f"   âš ï¸ é‡è¦ï¼šåªèƒ½é€‰æ‹©åˆ—è¡¨ä¸­çš„å€¼ï¼Œä¸èƒ½é€‰æ‹©å…¶ä»–å€¼æˆ–ä¸­é—´å€¼")
            
            if param.value_type:
                param_desc.append(f"   å€¼ç±»å‹: {param.value_type}")
            
            prompt_parts.append("\n".join(param_desc))
        
        return "\n".join(prompt_parts)
    
    def _build_objectives_prompt(self) -> str:
        """æ„å»ºä¼˜åŒ–ç›®æ ‡æè¿°æç¤ºè¯"""
        prompt_parts = [
            "# ä¼˜åŒ–ç›®æ ‡",
            "éœ€è¦ä¼˜åŒ–çš„æŒ‡æ ‡åŠå…¶æ–¹å‘ï¼š"
        ]
        
        for metric_name, config in self.objectives.items():
            minimize = config.get("minimize", True)
            direction = "æœ€å°åŒ–" if minimize else "æœ€å¤§åŒ–"
            direction_en = "minimize" if minimize else "maximize"
            prompt_parts.append(f"- **{metric_name}**: {direction} ({direction_en})")
        
        # æ·»åŠ ä¼˜åŒ–æ–¹å‘è¯´æ˜
        if len(self.objectives) > 1:
            prompt_parts.append("\n**æ³¨æ„**: è¿™æ˜¯å¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜ï¼Œéœ€è¦å¹³è¡¡å¤šä¸ªç›®æ ‡ã€‚")
        
        return "\n".join(prompt_parts)
    
    def _build_optimization_direction_instruction(self) -> str:
        """æ„å»ºä¼˜åŒ–æ–¹å‘è¯´æ˜"""
        instructions = []
        
        for metric_name, config in self.objectives.items():
            minimize = config.get("minimize", True)
            if minimize:
                instructions.append(
                    f"- **{metric_name}**: éœ€è¦**æœ€å°åŒ–**ï¼Œä¼˜å…ˆé€‰æ‹©èƒ½é™ä½è¯¥æŒ‡æ ‡å€¼çš„å‚æ•°ç»„åˆ"
                )
            else:
                instructions.append(
                    f"- **{metric_name}**: éœ€è¦**æœ€å¤§åŒ–**ï¼Œä¼˜å…ˆé€‰æ‹©èƒ½æé«˜è¯¥æŒ‡æ ‡å€¼çš„å‚æ•°ç»„åˆ"
                )
        
        return "\n".join(instructions)
    
    def _build_prior_data_prompt(self) -> str:
        """æ„å»ºå…ˆéªŒå®éªŒæ•°æ®æç¤ºè¯"""
        if not self.prior_experiments:
            return "# å…ˆéªŒå®éªŒæ•°æ®\næš‚æ— å…ˆéªŒå®éªŒæ•°æ®ã€‚"
        
        prompt_parts = [
            "# å…ˆéªŒå®éªŒæ•°æ®",
            f"ä»¥ä¸‹æ˜¯ {len(self.prior_experiments)} ä¸ªå†å²å®éªŒç»“æœï¼š",
            ""
        ]
        
        # è½¬æ¢ä¸ºè¡¨æ ¼æ ¼å¼
        data_rows = []
        for i, exp in enumerate(self.prior_experiments, 1):
            row = {
                "å®éªŒç¼–å·": i,
                **exp.parameters,
                **exp.metrics
            }
            data_rows.append(row)
        
        df = pd.DataFrame(data_rows)
        prompt_parts.append("```")
        prompt_parts.append(df.to_string(index=False))
        prompt_parts.append("```")
        
        
        return "\n".join(prompt_parts)
    
    def _build_optimization_prompt(self, num_suggestions: int = 1) -> str:
        """æ„å»ºå®Œæ•´çš„ä¼˜åŒ–æç¤ºè¯"""
        prompt_parts = [
            self._build_context_prompt(),
            "",
            self._build_parameter_space_prompt(),
            "",
            self._build_objectives_prompt(),
            "",
            self._build_prior_data_prompt(),
            "",
            "# ä¼˜åŒ–ä»»åŠ¡",
            f"åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œè¯·æ¨è {num_suggestions} ä¸ªå‚æ•°é…ç½®ç”¨äºä¸‹ä¸€æ­¥å®éªŒã€‚",
            "",
            "**ä¼˜åŒ–æ–¹å‘è¦æ±‚**ï¼š",
            self._build_optimization_direction_instruction(),
            "",
            "**å…¶ä»–è¦æ±‚**ï¼š",
            "1. **å‚æ•°å€¼å¿…é¡»ä¸¥æ ¼ç¬¦åˆå®šä¹‰**ï¼š",
            "   - å¯¹äºè¿ç»­å‚æ•°ï¼ˆrangeç±»å‹ï¼‰ï¼Œå€¼å¿…é¡»åœ¨ [æœ€å°å€¼, æœ€å¤§å€¼] èŒƒå›´å†…",
            "   - å¯¹äºç¦»æ•£å‚æ•°ï¼ˆchoiceç±»å‹ï¼‰ï¼Œå€¼å¿…é¡»**å®Œå…¨ç­‰äº**å¯é€‰å€¼åˆ—è¡¨ä¸­çš„æŸä¸ªå€¼ï¼Œä¸èƒ½æ˜¯å…¶ä»–å€¼",
            "   - ä¾‹å¦‚ï¼šå¦‚æœå¯é€‰å€¼æ˜¯ ['A', 'B', 'C', 'D']ï¼Œåˆ™åªèƒ½é€‰æ‹©è¿™4ä¸ªå€¼ä¹‹ä¸€",
            "2. å¦‚æœä½ æœ‰80%çš„æŠŠæ¡ä½ æ‰€æ¨èçš„å‚æ•°ç»„åˆå¾—åˆ°çš„å®éªŒç»“æœä¼šæ¯”å…ˆéªŒæ•°æ®ä¸­çš„æœ€å¥½çš„ç»“æœæ›´å¥½ï¼Œæ‰æ¨èè¿™ä¸ªå‚æ•°ç»„åˆã€‚",
            "3. è€ƒè™‘å…ˆéªŒæ•°æ®ä¸­çš„æ¨¡å¼å’Œè¶‹åŠ¿",
            "4. åœ¨æ¢ç´¢ï¼ˆexplorationï¼‰å’Œåˆ©ç”¨ï¼ˆexploitationï¼‰ä¹‹é—´å–å¾—å¹³è¡¡",
            "5. å¦‚æœå­˜åœ¨å¤šä¸ªç›®æ ‡ï¼Œéœ€è¦è€ƒè™‘å¤šç›®æ ‡ä¼˜åŒ–ï¼ˆå¸•ç´¯æ‰˜æœ€ä¼˜ï¼‰",
            "6. âš ï¸è­¦å‘Šï¼ä½ çš„æ¨èå¿…é¡»æ ¹æ®å…ˆéªŒæ•°æ®ä»¥åŠä½ æ‹¥æœ‰çš„è¡Œä¸šèƒŒæ™¯çŸ¥è¯†è¿›è¡Œæ¨ç†ï¼Œä¸èƒ½ç›´æ¥æ¨èå…ˆéªŒæ•°æ®ä¸­å·²æœ‰çš„ç‚¹ã€‚",
            "7. âš ï¸è­¦å‘Šï¼ä½ çš„æ¨èä¸èƒ½ä¸å…ˆéªŒæ•°æ®é‡å¤ã€‚æ¨èçš„å‚æ•°ç»„åˆä¸€å®šä¸èƒ½å·²å­˜åœ¨äºå…ˆéªŒæ•°æ®ä¸­",
            
            "",
            "è¯·ä»¥ JSON æ ¼å¼è¿”å›æ¨èçš„å‚æ•°é…ç½®ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š",
            "```json",
            "{",
            '  "suggestions": [',
            '    {',
            '      "å‚æ•°å1": å€¼1,',
            '      "å‚æ•°å2": å€¼2,',
            '      ...',
            '    }',
            '  ]',
            "}",
            "```"
        ]
        
        return "\n".join(prompt_parts)
    
    def _parse_llm_response(self, response: str) -> List[Dict[str, Any]]:
        """è§£æå¤§æ¨¡å‹è¿”å›çš„JSONæ ¼å¼å“åº”"""
        try:
            # å°è¯•æå–JSONéƒ¨åˆ†
            json_str = None
            
            # æ–¹æ³•1: æŸ¥æ‰¾ ```json ... ```
            if "```json" in response:
                json_start = response.find("```json") + 7
                json_end = response.find("```", json_start)
                if json_end > json_start:
                    json_str = response[json_start:json_end].strip()
            
            # æ–¹æ³•2: æŸ¥æ‰¾ ``` ... ```
            if json_str is None and "```" in response:
                parts = response.split("```")
                for i in range(1, len(parts), 2):
                    candidate = parts[i].strip()
                    if candidate.startswith("json"):
                        candidate = candidate[4:].strip()
                    if candidate.startswith("{") or candidate.startswith("["):
                        json_str = candidate
                        break
            
            # æ–¹æ³•3: æŸ¥æ‰¾ç¬¬ä¸€ä¸ª { æˆ– [
            if json_str is None:
                for start_char in ["{", "["]:
                    start_idx = response.find(start_char)
                    if start_idx >= 0:
                        # æ‰¾åˆ°åŒ¹é…çš„ç»“æŸå­—ç¬¦
                        end_char = "}" if start_char == "{" else "]"
                        depth = 0
                        for i in range(start_idx, len(response)):
                            if response[i] == start_char:
                                depth += 1
                            elif response[i] == end_char:
                                depth -= 1
                                if depth == 0:
                                    json_str = response[start_idx:i+1]
                                    break
                        if json_str:
                            break
            
            # æ–¹æ³•4: ç›´æ¥è§£ææ•´ä¸ªå“åº”
            if json_str is None:
                json_str = response.strip()
            
            result = json.loads(json_str)
            
            if "suggestions" in result:
                return result["suggestions"]
            elif isinstance(result, list):
                return result
            elif isinstance(result, dict):
                # å¦‚æœç»“æœæ˜¯å•ä¸ªå­—å…¸ï¼Œæ£€æŸ¥æ˜¯å¦åŒ…å«å‚æ•°
                if any(key in result for key in [p.name for p in self.parameters]):
                    return [result]
                else:
                    return []
            else:
                return []
        except json.JSONDecodeError as e:
            print(f"âš ï¸ JSONè§£æå¤±è´¥: {e}")
            print(f"å“åº”å†…å®¹ï¼ˆå‰500å­—ç¬¦ï¼‰: {response[:500]}")
            # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨
            return []
        except Exception as e:
            print(f"âš ï¸ è§£æå“åº”æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return []
    
    def _validate_parameters(self, params: Dict[str, Any]) -> bool:
        """éªŒè¯å‚æ•°æ˜¯å¦åœ¨å®šä¹‰çš„ç©ºé—´å†…"""
        for param_def in self.parameters:
            param_name = param_def.name
            if param_name not in params:
                continue  # å…è®¸ç¼ºå°‘æŸäº›å‚æ•°
            
            value = params[param_name]
            
            if param_def.type == "range":
                if param_def.bounds is None:
                    continue
                min_val, max_val = param_def.bounds
                if not (min_val <= value <= max_val):
                    return False
                
                # ç±»å‹è½¬æ¢æ£€æŸ¥
                if param_def.value_type == "int":
                    if not isinstance(value, (int, float)) or int(value) != value:
                        return False
            elif param_def.type == "choice":
                if param_def.values is None:
                    continue
                if value not in param_def.values:
                    return False
        
        return True
    
    def _normalize_parameters(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """è§„èŒƒåŒ–å‚æ•°å€¼ï¼ˆç±»å‹è½¬æ¢ç­‰ï¼‰"""
        normalized = {}
        
        for param_def in self.parameters:
            param_name = param_def.name
            if param_name not in params:
                continue
            
            value = params[param_name]
            
            # ç±»å‹è½¬æ¢
            if param_def.value_type == "int":
                value = int(float(value))
            elif param_def.value_type == "float":
                value = float(value)
            elif param_def.value_type == "str":
                value = str(value)
            
            normalized[param_name] = value
        
        return normalized
    
    def suggest_parameters(self, num_suggestions: int = 1, print_prompt: bool = False, print_response: bool = False) -> List[Dict[str, Any]]:
        """
        ç”Ÿæˆå‚æ•°ä¼˜åŒ–å»ºè®®
        
        Args:
            num_suggestions: éœ€è¦ç”Ÿæˆçš„å»ºè®®æ•°é‡
            print_prompt: æ˜¯å¦æ‰“å°è¾“å…¥å¤§æ¨¡å‹çš„å®Œæ•´æç¤ºè¯
            print_response: æ˜¯å¦æ‰“å°å¤§æ¨¡å‹çš„åŸå§‹å›ç­”
            
        Returns:
            æ¨èçš„å‚æ•°é…ç½®åˆ—è¡¨
        """
        # æ„å»ºæç¤ºè¯
        prompt = self._build_optimization_prompt(num_suggestions=num_suggestions)
        
        # æ ¹æ® print_prompt å‚æ•°å†³å®šæ˜¯å¦æ‰“å°å®Œæ•´æç¤ºè¯
        if print_prompt:
            print("\n" + "=" * 80)
            print("ğŸ“ è¾“å…¥å¤§æ¨¡å‹çš„å®Œæ•´æç¤ºè¯:")
            print("=" * 80)
            print(prompt)
            print("=" * 80 + "\n")
        
        # è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå»ºè®®
        print("ğŸ¤– æ­£åœ¨ä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆä¼˜åŒ–å»ºè®®...")
        response = self.llm_provider.generate(prompt)
        
        # æ ¹æ® print_response å‚æ•°å†³å®šæ˜¯å¦æ‰“å°å¤§æ¨¡å‹çš„åŸå§‹å›ç­”
        if print_response:
            print("\n" + "=" * 80)
            print("ğŸ“¤ å¤§æ¨¡å‹çš„åŸå§‹å›ç­”:")
            print("=" * 80)
            print(response)
            print("=" * 80 + "\n")
        
        # è§£æå“åº”
        suggestions = self._parse_llm_response(response)
        
        # éªŒè¯å’Œè§„èŒƒåŒ–å‚æ•°
        valid_suggestions = []
        for suggestion in suggestions:
            # ç§»é™¤ reasoning å­—æ®µï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            params = {k: v for k, v in suggestion.items() if k != "reasoning"}
            
            if self._validate_parameters(params):
                normalized = self._normalize_parameters(params)
                valid_suggestions.append(normalized)
            else:
                print(f"âš ï¸ å‚æ•°éªŒè¯å¤±è´¥ï¼Œè·³è¿‡: {params}")
        
        # å¦‚æœéªŒè¯åçš„å»ºè®®æ•°é‡ä¸è¶³ï¼Œå°è¯•ç”Ÿæˆæ›´å¤š
        if len(valid_suggestions) < num_suggestions:
            print(f"âš ï¸ åªç”Ÿæˆäº† {len(valid_suggestions)} ä¸ªæœ‰æ•ˆå»ºè®®ï¼ŒæœŸæœ› {num_suggestions} ä¸ª")
        
        # è®°å½•åˆ°å†å²
        for suggestion in valid_suggestions:
            self.optimization_history.append({
                "suggestion": suggestion,
                "timestamp": pd.Timestamp.now().isoformat()
            })
        
        return valid_suggestions[:num_suggestions]
    
    def add_experiment_result(
        self,
        parameters: Dict[str, Any],
        metrics: Dict[str, float],
        metadata: Optional[Dict[str, Any]] = None
    ):
        """æ·»åŠ å®éªŒç»“æœåˆ°å…ˆéªŒæ•°æ®"""
        experiment = PriorExperiment(
            parameters=parameters,
            metrics=metrics,
            metadata=metadata
        )
        self.prior_experiments.append(experiment)
    



def example_usage():
    """ä½¿ç”¨ç¤ºä¾‹"""
    # 1. å®šä¹‰é—®é¢˜èƒŒæ™¯
    problem_context = ProblemContext(
        problem_description="ä¼˜åŒ–æ¿€å…‰åˆ‡å‰²å·¥è‰ºå‚æ•°ï¼Œä»¥æé«˜åˆ‡å‰²è´¨é‡å’Œæ•ˆç‡",
        industry="åˆ¶é€ ä¸š - æ¿€å…‰åŠ å·¥",
        domain_knowledge="æ¿€å…‰åŠŸç‡ã€åˆ‡å‰²é€Ÿåº¦å’Œé¢‘ç‡å¯¹è¡¨é¢ç²—ç³™åº¦å’Œåˆ‡ç¼å®½åº¦æœ‰æ˜¾è‘—å½±å“",
        constraints=["åŠŸç‡ä¸èƒ½è¶…è¿‡è®¾å¤‡ä¸Šé™", "é€Ÿåº¦å¿…é¡»ä¿è¯åˆ‡å‰²è´¨é‡"],
        optimization_goals=["æœ€å°åŒ–è¡¨é¢ç²—ç³™åº¦", "æœ€å°åŒ–åˆ‡ç¼å®½åº¦"]
    )
    
    # 2. å®šä¹‰å‚æ•°ç©ºé—´
    parameter_space = [
        Parameter(
            name="power",
            type="range",
            bounds=[1000, 3000],
            value_type="int",
            description="æ¿€å…‰åŠŸç‡",
            unit="W"
        ),
        Parameter(
            name="speed",
            type="range",
            bounds=[10.0, 50.0],
            value_type="float",
            description="åˆ‡å‰²é€Ÿåº¦",
            unit="mm/s"
        ),
        Parameter(
            name="frequency",
            type="choice",
            values=[500, 1000, 1500, 2000],
            value_type="int",
            description="è„‰å†²é¢‘ç‡",
            unit="Hz"
        )
    ]
    
    # 3. å®šä¹‰ä¼˜åŒ–ç›®æ ‡
    # æ ¼å¼: {"metric_name": {"minimize": bool}}
    # minimize=True è¡¨ç¤ºæœ€å°åŒ–ï¼Œminimize=False è¡¨ç¤ºæœ€å¤§åŒ–
    objectives = {
        "roughness": {"minimize": True},  # æœ€å°åŒ–è¡¨é¢ç²—ç³™åº¦
        "kerf_width": {"minimize": True}  # æœ€å°åŒ–åˆ‡ç¼å®½åº¦
        # ç¤ºä¾‹ï¼šå¦‚æœè¦æœ€å¤§åŒ–æŸä¸ªæŒ‡æ ‡ï¼Œå¯ä»¥è®¾ç½®ï¼š
        # "efficiency": {"minimize": False}  # æœ€å¤§åŒ–æ•ˆç‡
    }
    
    # 4. å®šä¹‰å…ˆéªŒå®éªŒæ•°æ®ï¼ˆå¯é€‰ï¼‰
    prior_experiments = [
        PriorExperiment(
            parameters={"power": 2000, "speed": 30.0, "frequency": 1000},
            metrics={"roughness": 2.5, "kerf_width": 0.15}
        ),
        PriorExperiment(
            parameters={"power": 2500, "speed": 40.0, "frequency": 1500},
            metrics={"roughness": 1.8, "kerf_width": 0.18}
        )
    ]
    
    # 5. åˆ›å»º LLINBO Agent
    # æ³¨æ„ï¼šéœ€è¦è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡æˆ–æä¾› api_key
    llm_config = LLMConfig(
        model_name="gpt-5-mini",
        api_key=os.getenv("OPENAI_API_KEY"),
        base_url="https://api.openai.com/v1"
    )
    
    agent = LLINBOAgent(
        problem_context=problem_context,
        parameters = parameter_space,
        objectives=objectives,
        llm_config=llm_config,
        prior_experiments=prior_experiments
    )
    
    # 6. ç”Ÿæˆä¼˜åŒ–å»ºè®®
    suggestions = agent.suggest_parameters(num_suggestions=3, print_prompt=True, print_response=True)
    
    print("\nğŸ“Š ç”Ÿæˆçš„ä¼˜åŒ–å»ºè®®:")
    print(suggestions)
 


if __name__ == "__main__":
    example_usage()

