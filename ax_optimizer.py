from typing import List, Dict, Any, Optional, Tuple, Type
from ax.service.ax_client import AxClient, ObjectiveProperties
from ax.generation_strategy.generation_strategy import GenerationStrategy, GenerationStep
from ax.modelbridge.registry import Generators
from ax.core.objective import ScalarizedObjective, MultiObjective, Objective
from ax.core.metric import Metric

# æ–°å¢å¯¼å…¥ä»¥æ”¯æŒè‡ªå®šä¹‰ä»£ç†æ¨¡å‹ã€æ ¸å‡½æ•°å’Œé‡‡é›†å‡½æ•°
from ax.models.torch.botorch_modular.surrogate import SurrogateSpec, ModelConfig
from ax.models.torch.botorch_modular.acquisition import Acquisition
from botorch.models import (
    SingleTaskGP, MultiTaskGP, KroneckerMultiTaskGP, 
    MixedSingleTaskGP, SingleTaskMultiFidelityGP,
    SaasFullyBayesianSingleTaskGP, SaasFullyBayesianMultiTaskGP,
    HigherOrderGP, SingleTaskVariationalGP
)
from gpytorch.kernels import (
    RBFKernel, MaternKernel, ScaleKernel, LinearKernel,
    PolynomialKernel, PeriodicKernel, SpectralMixtureKernel,
    RQKernel, PiecewisePolynomialKernel, AdditiveKernel,
    ProductKernel, CosineKernel
)
# é‡‡é›†å‡½æ•°å¯¼å…¥
from botorch.acquisition.acquisition import AcquisitionFunction
from botorch.acquisition.monte_carlo import (
    qExpectedImprovement, qNoisyExpectedImprovement, qUpperConfidenceBound
)
from botorch.acquisition.analytic import (
    ExpectedImprovement, UpperConfidenceBound, PosteriorMean
)
from botorch.acquisition.knowledge_gradient import (
    qKnowledgeGradient, qMultiFidelityKnowledgeGradient
)
from botorch.acquisition.max_value_entropy_search import (
    qMaxValueEntropy, qMultiFidelityMaxValueEntropy
)
from botorch.acquisition.logei import (
    qLogExpectedImprovement, qLogNoisyExpectedImprovement
)
# å¤šç›®æ ‡é‡‡é›†å‡½æ•°
from botorch.acquisition.multi_objective.monte_carlo import (
    qExpectedHypervolumeImprovement, qNoisyExpectedHypervolumeImprovement
)
from botorch.acquisition.multi_objective.logei import (
    qLogExpectedHypervolumeImprovement, qLogNoisyExpectedHypervolumeImprovement
)
from botorch.acquisition.multi_objective.parego import qLogNParEGO
        
import numpy as np
import pandas as pd
from dataclasses import dataclass

@dataclass
class ExperimentResult:
    """å®éªŒç»“æœæ•°æ®ç±»"""
    parameters: Dict[str, Any]  # å‚æ•°é…ç½®
    metrics: Dict[str, float]   # æŒ‡æ ‡ç»“æœ
    metadata: Optional[Dict[str, Any]] = None  # é¢å¤–å…ƒæ•°æ®

class BayesianOptimizer:
    def __init__(
        self,
        search_space: List[Dict[str, Any]],
        optimization_config: Dict[str, Any],
        experiment_name: str = "bayesian_optimization",
        random_seed: Optional[int] = None,
        # æ–°å¢ï¼šå¯é€‰çš„ä»£ç†æ¨¡å‹å’Œæ ¸å‡½æ•°é…ç½®
        surrogate_model_class: Optional[Type] = None,
        kernel_class: Optional[Type] = None,
        kernel_options: Optional[Dict[str, Any]] = None,
        # æ–°å¢ï¼šå¯é€‰çš„é‡‡é›†å‡½æ•°é…ç½®
        acquisition_function_class: Optional[Type[AcquisitionFunction]] = None,
        acquisition_function_options: Optional[Dict[str, Any]] = None
    ):
        """
        åˆå§‹åŒ–è´å¶æ–¯ä¼˜åŒ–å™¨
        
        Args:
            search_space: å‚æ•°æœç´¢ç©ºé—´é…ç½®åˆ—è¡¨ï¼Œæ¯ä¸ªå‚æ•°éœ€åŒ…å«ï¼š
                - name: å‚æ•°å
                - type: "range"ï¼ˆè¿ç»­ï¼‰æˆ–"choice"ï¼ˆç¦»æ•£ï¼‰
                - bounds: [min, max] ï¼ˆtypeä¸ºrangeæ—¶ï¼‰
                - values: list ï¼ˆtypeä¸ºchoiceæ—¶ï¼‰
                - value_type: "int"æˆ–"float"æˆ–"str"
            optimization_config: ä¼˜åŒ–é…ç½®ï¼ŒåŒ…å«ï¼š
                - objectives: ä¼˜åŒ–ç›®æ ‡å­—å…¸ï¼Œæ ¼å¼ä¸º {"metric_name": {"minimize": bool}}
                - use_weights: æ˜¯å¦ä½¿ç”¨æƒé‡ï¼ˆå¯é€‰ï¼Œé»˜è®¤Falseï¼‰
                - objective_weights: ç›®æ ‡æƒé‡å­—å…¸ï¼Œæ ¼å¼ä¸º {"metric_name": weight}ï¼ˆå¯é€‰ï¼‰
                - additional_metrics: å…¶ä»–éœ€è¦è®°å½•çš„æŒ‡æ ‡åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            experiment_name: å®éªŒåç§°
            random_seed: éšæœºç§å­
            surrogate_model_class: ä»£ç†æ¨¡å‹ç±»ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨Axé»˜è®¤æ¨¡å‹ï¼‰
            kernel_class: æ ¸å‡½æ•°ç±»ï¼ˆå¯é€‰ï¼Œå¦‚MaternKernelã€RBFKernelç­‰ï¼‰
            kernel_options: æ ¸å‡½æ•°å‚æ•°ï¼ˆå¯é€‰ï¼Œå¦‚{"nu": 2.5}ç”¨äºMaternKernelï¼‰
            acquisition_function_class: é‡‡é›†å‡½æ•°ç±»ï¼ˆå¯é€‰ï¼Œå¦‚qExpectedImprovementã€qUpperConfidenceBoundç­‰ï¼‰
            acquisition_function_options: é‡‡é›†å‡½æ•°å‚æ•°ï¼ˆå¯é€‰ï¼Œå¦‚{"beta": 0.1}ç”¨äºUCBï¼‰
        """
        self.experiment_name = experiment_name
        self.random_seed = random_seed
        
        # å¤„ç†å­—ç¬¦ä¸²åˆ°ç±»çš„è½¬æ¢
        def convert_string_to_class(class_obj):
            """å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºç±»å¯¹è±¡"""
            if isinstance(class_obj, str):
                import sys
                import os
                sys.path.append(os.path.dirname(__file__))
                from __init__ import get_class_from_string
                return get_class_from_string(class_obj)
            return class_obj
        
        surrogate_model_class = convert_string_to_class(surrogate_model_class)
        kernel_class = convert_string_to_class(kernel_class)
        acquisition_function_class = convert_string_to_class(acquisition_function_class)
        
        # åˆå§‹åŒ–AxClientï¼Œç›´æ¥ä½¿ç”¨BOTORCHæ¨¡å‹

        # åˆ›å»ºåªä½¿ç”¨BOTORCH_MODULARçš„ç”Ÿæˆç­–ç•¥ï¼Œæ”¯æŒè‡ªå®šä¹‰ä»£ç†æ¨¡å‹ã€æ ¸å‡½æ•°å’Œé‡‡é›†å‡½æ•°
        model_kwargs = {}
        
        # å¦‚æœæŒ‡å®šäº†è‡ªå®šä¹‰ä»£ç†æ¨¡å‹æˆ–æ ¸å‡½æ•°ï¼Œåˆ›å»ºSurrogateSpec
        if surrogate_model_class is not None or kernel_class is not None:
            # è®¾ç½®é»˜è®¤å€¼
            model_class = surrogate_model_class or None
            kernel_cls = kernel_class or None
            kernel_opts = kernel_options or {}
            
            # åˆ›å»ºæ¨¡å‹é…ç½®
            model_config = ModelConfig(
                botorch_model_class=model_class,
                covar_module_class=kernel_cls,
                covar_module_options=kernel_opts,
            )
            
            # åˆ›å»ºä»£ç†è§„æ ¼
            surrogate_spec = SurrogateSpec(model_configs=[model_config])
            model_kwargs["surrogate_spec"] = surrogate_spec
        
        # å¦‚æœæŒ‡å®šäº†è‡ªå®šä¹‰é‡‡é›†å‡½æ•°ï¼Œæ·»åŠ åˆ°model_kwargsä¸­
        if acquisition_function_class is not None:
            model_kwargs["botorch_acqf_class"] = acquisition_function_class
        
        # å¦‚æœæŒ‡å®šäº†é‡‡é›†å‡½æ•°é€‰é¡¹ï¼Œæ·»åŠ åˆ°model_kwargsä¸­
        if acquisition_function_options is not None:
            if "acquisition_options" not in model_kwargs:
                model_kwargs["acquisition_options"] = {}
            model_kwargs["acquisition_options"].update(acquisition_function_options)
        
        gs = GenerationStrategy(
            name="BOTORCH_MODULAR",
            steps=[
                GenerationStep(
                    model=Generators.BOTORCH_MODULAR,
                    num_trials=-1,  # -1è¡¨ç¤ºæ— é™æ¬¡æ•°
                    model_kwargs=model_kwargs if model_kwargs else None
                ),
            ]
        )
        
        self.ax_client = AxClient(
            random_seed=random_seed,
            generation_strategy=gs
        )
        
        # æ„å»ºä¼˜åŒ–ç›®æ ‡é…ç½®
        objectives = optimization_config.get("objectives", {})
        if not objectives:
            raise ValueError("optimization_config å¿…é¡»åŒ…å« 'objectives' é…ç½®")
        
        # æ£€æŸ¥æƒé‡è®¾ç½®
        use_weights = optimization_config.get("use_weights", False)
        objective_weights = optimization_config.get("objective_weights", {})
        
        # æ„å»ºObjectivePropertieså­—å…¸ï¼ˆAxæœŸæœ›çš„æ ¼å¼ï¼‰
        objective_properties = {}
        for metric_name, config in objectives.items():
            minimize = config.get("minimize", True)
            objective_properties[metric_name] = ObjectiveProperties(minimize=minimize)
        
        # åˆ›å»ºå®éªŒ
        self.ax_client.create_experiment(
            name=experiment_name,
            parameters=search_space,
            objectives=objective_properties,
            tracking_metric_names=optimization_config.get("additional_metrics", [])
        )
        
        # å¦‚æœä½¿ç”¨æƒé‡ï¼Œåœ¨åˆ›å»ºå®éªŒåè®¾ç½®ScalarizedObjective
        if use_weights and len(objectives) > 1:
            try:
                # è·å–å·²åˆ›å»ºçš„å®éªŒ
                experiment = self.ax_client.experiment
                
                # åˆ›å»ºScalarizedObjective
                metrics = []
                weights = []
                minimize = None
                
                for metric_name, config in objectives.items():
                    # ä»å®éªŒä¸­è·å–å·²åˆ›å»ºçš„Metric
                    metric = experiment.metrics[metric_name]
                    metrics.append(metric)
                    
                    weight = objective_weights.get(metric_name, 1.0)
                    weights.append(weight)
                    
                    # æ£€æŸ¥ä¼˜åŒ–æ–¹å‘æ˜¯å¦ä¸€è‡´
                    current_minimize = config.get("minimize", True)
                    if minimize is None:
                        minimize = current_minimize
                    elif minimize != current_minimize:
                        raise ValueError(f"çº¿æ€§åŠ æƒä¼˜åŒ–è¦æ±‚æ‰€æœ‰ç›®æ ‡ä½¿ç”¨ç›¸åŒçš„ä¼˜åŒ–æ–¹å‘ï¼Œä½† {metric_name} çš„æ–¹å‘ä¸å…¶ä»–ç›®æ ‡ä¸ä¸€è‡´")
                
                # åˆ›å»ºScalarizedObjectiveå¹¶è®¾ç½®åˆ°å®éªŒ
                scalarized_objective = ScalarizedObjective(
                    metrics=metrics,
                    weights=weights,
                    minimize=minimize
                )
                
                # æ›´æ–°å®éªŒçš„ä¼˜åŒ–é…ç½®
                from ax.core.optimization_config import OptimizationConfig
                experiment.optimization_config = OptimizationConfig(objective=scalarized_objective)
                
            except Exception as e:
                import logging
                logger = logging.getLogger(__name__)
                logger.warning(f"è®¾ç½®ScalarizedObjectiveå¤±è´¥ï¼Œå°†ä½¿ç”¨æ™®é€šå¤šç›®æ ‡ä¼˜åŒ–: {e}")
        
        # è®°å½•å®éªŒæ¬¡æ•°
        self.trial_count = 0
        
    def add_prior_experiments(self, experiments: List[ExperimentResult]) -> None:
        """
        æ·»åŠ å…ˆéªŒå®éªŒæ•°æ®
        
        Args:
            experiments: å…ˆéªŒå®éªŒç»“æœåˆ—è¡¨
        """
        for exp in experiments:
            # ç›´æ¥ä½¿ç”¨åŸå§‹å‚æ•°å€¼ï¼Œè®©Axè‡ªå·±å¤„ç†ç±»å‹è½¬æ¢
            parameters = exp.parameters.copy()
            
            # å…ˆåˆ›å»ºè¯•éªŒ
            _, trial_index = self.ax_client.attach_trial(parameters)
            
            # æ›´æ–°è¯•éªŒç»“æœ
            raw_data = {
                metric_name: (metric_value, 0.0)  # (value, SEM)
                for metric_name, metric_value in exp.metrics.items()
            }
            self.ax_client.complete_trial(
                trial_index=trial_index,
                raw_data=raw_data,
                metadata=exp.metadata
            )
    
    def get_next_parameters(self, n: int = 1) -> List[Tuple[Dict[str, Any], int]]:
        """
        è·å–ä¸‹ä¸€ç»„æˆ–å¤šç»„å»ºè®®çš„å‚æ•°é…ç½®
        
        Args:
            n: éœ€è¦ç”Ÿæˆçš„å‚æ•°ç»„æ•°ï¼Œé»˜è®¤ä¸º1
            
        Returns:
            List of (parameters, trial_index) tuples:
            - parameters: å‚æ•°é…ç½®å­—å…¸
            - trial_index: å®éªŒç´¢å¼•
        """
        self.trial_count += n
        results = []
        for _ in range(n):
            parameters, trial_index = self.ax_client.get_next_trial()
            results.append((parameters, trial_index))
        return results
    
    def update_experiment(self, 
                         trial_index: int,
                         metrics: Dict[str, float],
                         metadata: Optional[Dict[str, Any]] = None) -> None:
        """
        æ›´æ–°å®éªŒç»“æœ
        
        Args:
            trial_index: å®éªŒç´¢å¼•
            metrics: æŒ‡æ ‡ç»“æœå­—å…¸
            metadata: é¢å¤–å…ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰
        """
        raw_data = {
            metric_name: (metric_value, 0.0)  # (value, SEM)
            for metric_name, metric_value in metrics.items()
        }
        self.ax_client.complete_trial(
            trial_index=trial_index,
            raw_data=raw_data,
            metadata=metadata
        )
    
    def get_best_parameters(self) -> Tuple[Dict[str, Any], Dict[str, float]]:
        """
        è·å–å½“å‰æœ€ä¼˜å‚æ•°é…ç½®å’Œå¯¹åº”çš„æŒ‡æ ‡å€¼
        è‡ªåŠ¨å¤„ç†ä¸‰ç§ä¼˜åŒ–ç±»å‹ï¼š
        - å•ç›®æ ‡ä¼˜åŒ– (Objective): è¿”å›æœ€ä¼˜è§£
        - å¸•ç´¯æ‰˜å¤šç›®æ ‡ä¼˜åŒ– (MultiObjective): ä½¿ç”¨Axè¿”å›çš„å¸•ç´¯æ‰˜å‰æ²¿é›†åˆï¼Œé€‰æ‹©ä¸€ä¸ªä»£è¡¨æ€§è§£
        - çº¿æ€§åŠ æƒå¤šç›®æ ‡ä¼˜åŒ– (ScalarizedObjective): è¿”å›åŠ æƒæœ€ä¼˜è§£
        
        Returns:
            best_parameters: æœ€ä¼˜å‚æ•°é…ç½®
            best_metrics: æœ€ä¼˜æŒ‡æ ‡å€¼
        """
        try:
            # å¦‚æœæ˜¯å¤šç›®æ ‡ï¼ˆMOOï¼‰ï¼ŒAxè¦æ±‚ä½¿ç”¨get_pareto_optimal_parameters
            opt_config = self.ax_client.experiment.optimization_config
            if opt_config is not None and getattr(opt_config, "is_moo_problem", False):
                pareto = self.ax_client.get_pareto_optimal_parameters(use_model_predictions=True)
                if not pareto:
                    return {}, {}
                # é€‰æ‹©ä¸€ä¸ªä»£è¡¨æ€§è§£ï¼šæŒ‰trial_indexä»å°åˆ°å¤§å–ç¬¬ä¸€ä¸ª
                chosen_index = sorted(pareto.keys())[0]
                parameters, metric_values = pareto[chosen_index]
                means, _ = metric_values
                metrics = {name: (val, 0.0) for name, val in means.items()}
                return parameters, metrics

            # ç›´æ¥ä½¿ç”¨Axçš„get_best_trialæ–¹æ³•ï¼ŒAxä¼šè‡ªåŠ¨å¤„ç†ä¸åŒçš„ä¼˜åŒ–ç±»å‹
            best_trial = self.ax_client.get_best_trial()
            if best_trial is None:
                return {}, {}
                
            # è§£åŒ…è¿”å›çš„ä¸‰å…ƒç»„ï¼š(trial_index, parameters, metrics)
            _, parameters, metric_values = best_trial
            
            # æ„å»ºæŒ‡æ ‡å­—å…¸
            metrics = {}
            # metric_values æ˜¯ä¸€ä¸ªå…ƒç»„ï¼š(means, covariances)
            means, _ = metric_values
            for metric_name, mean_value in means.items():
                metrics[metric_name] = (mean_value, 0.0)  # ä½¿ç”¨0.0ä½œä¸ºsemï¼Œå› ä¸ºæˆ‘ä»¬æ²¡æœ‰ä½¿ç”¨åæ–¹å·®
                
            return parameters, metrics
                
        except Exception as e:
            import logging
            logger = logging.getLogger(__name__)
            logger.error(f"è·å–æœ€ä¼˜å‚æ•°å¤±è´¥: {e}")
            # å¦‚æœget_best_trialå¤±è´¥ï¼Œå°è¯•ä»å†å²æ•°æ®ä¸­è·å–æœ€ä¼˜è§£
            try:
                # å†æ¬¡ä¼˜å…ˆå¤„ç†MOOï¼šç›´æ¥è¿”å›å¸•ç´¯æ‰˜é›†åˆä¸­çš„ä¸€ä¸ªä»£è¡¨æ€§è§£
                opt_config = self.ax_client.experiment.optimization_config
                if opt_config is not None and getattr(opt_config, "is_moo_problem", False):
                    pareto = self.ax_client.get_pareto_optimal_parameters(use_model_predictions=True)
                    if pareto:
                        chosen_index = sorted(pareto.keys())[0]
                        parameters, metric_values = pareto[chosen_index]
                        means, _ = metric_values
                        metrics = {name: (val, 0.0) for name, val in means.items()}
                        return parameters, metrics

                trials_df = self.ax_client.get_trials_data_frame()
                if trials_df.empty:
                    return {}, {}
                
                # è·å–ä¼˜åŒ–ç›®æ ‡ç±»å‹
                objective = self.ax_client.experiment.optimization_config.objective
                
                # æ‰¾åˆ°æœ€ä¼˜çš„trial
                best_idx = None
                
                if isinstance(objective, ScalarizedObjective):
                    # åŠ æƒä¼˜åŒ–ï¼šç›´æ¥æ‰¾objective_meanåˆ—çš„æœ€ä¼˜å€¼
                    if 'objective_mean' in trials_df.columns:
                        minimize = getattr(objective, 'minimize', True)
                        best_value = float('inf') if minimize else float('-inf')
                        
                        for idx, trial in trials_df.iterrows():
                            value = trial.get('objective_mean')
                            if pd.isna(value):
                                continue
                            if minimize:
                                if value < best_value:
                                    best_value = value
                                    best_idx = idx
                            else:
                                if value > best_value:
                                    best_value = value
                                    best_idx = idx
                    else:
                        return {}, {}  # æ²¡æœ‰objective_meanåˆ—ï¼Œæ— æ³•å¤„ç†
                else:
                    # éåŠ æƒä¼˜åŒ–ï¼šä½¿ç”¨ç¬¬ä¸€ä¸ªç›®æ ‡
                    if hasattr(objective, 'metric_names'):
                        objective_names = list(objective.metric_names)
                    else:
                        objective_names = [col[:-5] for col in trials_df.columns if col.endswith('_mean') and col != 'objective_mean']
                    
                    if not objective_names:
                        return {}, {}
                    
                    target_col = f"{objective_names[0]}_mean"
                    minimize = getattr(objective, 'minimize', True)
                    best_value = float('inf') if minimize else float('-inf')
                    
                    for idx, trial in trials_df.iterrows():
                        value = trial.get(target_col)
                        if pd.isna(value):
                            continue
                        if minimize:
                            if value < best_value:
                                best_value = value
                                best_idx = idx
                        else:
                            if value > best_value:
                                best_value = value
                                best_idx = idx
                
                if best_idx is not None:
                    trial = trials_df.iloc[best_idx]
                    parameters = {}
                    metrics = {}
                    
                    # æå–å‚æ•°
                    for param_name in self.ax_client.experiment.search_space.parameters.keys():
                        if f"{param_name}" in trial:
                            parameters[param_name] = trial[f"{param_name}"]
                    
                    # æå–æŒ‡æ ‡
                    if isinstance(objective, ScalarizedObjective):
                        # åŠ æƒä¼˜åŒ–ï¼šæå–åŸå§‹æŒ‡æ ‡å€¼
                        for metric in getattr(objective, 'metrics', []):
                            metric_name = metric.name if hasattr(metric, 'name') else str(metric)
                            if f"{metric_name}_mean" in trial:
                                metrics[metric_name] = (trial[f"{metric_name}_mean"], trial.get(f"{metric_name}_sem", 0.0))
                    else:
                        # éåŠ æƒä¼˜åŒ–ï¼šæå–ç›®æ ‡æŒ‡æ ‡å€¼
                        if hasattr(objective, 'metric_names'):
                            objective_names = list(objective.metric_names)
                        else:
                            objective_names = [col[:-5] for col in trials_df.columns if col.endswith('_mean') and col != 'objective_mean']
                        
                        for name in objective_names:
                            if f"{name}_mean" in trial:
                                metrics[name] = (trial[f"{name}_mean"], trial.get(f"{name}_sem", 0.0))
                    
                    return parameters, metrics
                
            except Exception as e2:
                logger.error(f"ä»å†å²æ•°æ®è·å–æœ€ä¼˜å‚æ•°ä¹Ÿå¤±è´¥: {e2}")
            
            return {}, {}
    
    def get_optimization_history(self) -> pd.DataFrame:
        """
        è·å–ä¼˜åŒ–å†å²è®°å½•
        
        Returns:
            history_df: åŒ…å«æ‰€æœ‰å®éªŒè®°å½•çš„DataFrame
        """
        return self.ax_client.get_trials_data_frame()

def test_optimizer():
    """
    æµ‹è¯•è´å¶æ–¯ä¼˜åŒ–å™¨çš„åŸºæœ¬åŠŸèƒ½
    åŒ…æ‹¬é»˜è®¤é…ç½®ã€è‡ªå®šä¹‰ä»£ç†æ¨¡å‹/æ ¸å‡½æ•°é…ç½®å’Œè‡ªå®šä¹‰é‡‡é›†å‡½æ•°é…ç½®
    """
    # å®šä¹‰æœç´¢ç©ºé—´
    search_space = [
        {
            "name": "power",
            "type": "range",
            "bounds": [1000, 3000],
            "value_type": "int",
            "log_scale": False  # çº¿æ€§å°ºåº¦å¯èƒ½æ›´é€‚åˆåŠŸç‡å‚æ•°
        },
        {
            "name": "speed",
            "type": "range",
            "bounds": [10, 50],
            "value_type": "float",
            "log_scale": False  # çº¿æ€§å°ºåº¦é€‚åˆé€Ÿåº¦å‚æ•°
        },
        {
            "name": "frequency",
            "type": "choice",
            "values": [500, 1000, 1500, 2000],
            "value_type": "int",
            "is_ordered": True,
            "sort_values": True
        }
    ]
    
    # ä¼˜åŒ–é…ç½® - å¸•ç´¯æ‰˜å¤šç›®æ ‡ä¼˜åŒ–
    optimization_config = {
        "objectives": {
            "roughness": {"minimize": True},  # æœ€å°åŒ–è¡¨é¢ç²—ç³™åº¦
            "kerf_width": {"minimize": True}  # æœ€å°åŒ–åˆ‡ç¼å®½åº¦
        },
        "use_weights": False,  # ä¸å¯ç”¨æƒé‡ï¼Œä½¿ç”¨å¸•ç´¯æ‰˜ä¼˜åŒ–
        "additional_metrics": []  # æ²¡æœ‰é¢å¤–çš„è·Ÿè¸ªæŒ‡æ ‡
    }
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = BayesianOptimizer(
        search_space=search_space,
        optimization_config=optimization_config,
        experiment_name="laser_cutting_optimization",
        random_seed=42
    )
    
    # æ·»åŠ ä¸€äº›å…ˆéªŒå®éªŒæ•°æ®
    prior_experiments = [
        ExperimentResult(
            parameters={"power": 2000, "speed": 30.0, "frequency": 1000},  # ç¡®ä¿speedæ˜¯floatç±»å‹
            metrics={"roughness": 2.5, "kerf_width": 0.15}
        ),
        ExperimentResult(
            parameters={"power": 2500, "speed": 40.0, "frequency": 1500},  # ç¡®ä¿speedæ˜¯floatç±»å‹
            metrics={"roughness": 1.8, "kerf_width": 0.18}
        )
    ]
    optimizer.add_prior_experiments(prior_experiments)
    
    # æ¨¡æ‹Ÿä¼˜åŒ–è¿‡ç¨‹
    # ä¸€æ¬¡æ€§è·å–3ç»„å‚æ•°
    next_trials = optimizer.get_next_parameters(n=3)
    for i, (parameters, trial_index) in enumerate(next_trials):
        print(f"\nç¬¬ {i+1} æ¬¡å®éªŒå‚æ•°ï¼š")
        print(parameters)
        
        # æ¨¡æ‹Ÿå®éªŒç»“æœï¼ˆè¿™é‡Œç”¨éšæœºå€¼ä»£æ›¿å®é™…å®éªŒï¼‰
        roughness = np.random.uniform(1.5, 3.0)
        kerf_width = np.random.uniform(0.1, 0.2)
        
        # æ›´æ–°å®éªŒç»“æœ
        optimizer.update_experiment(
            trial_index=trial_index,
            metrics={"roughness": roughness, "kerf_width": kerf_width}
        )
    
    # è·å–æœ€ä¼˜ç»“æœï¼ˆå¸•ç´¯æ‰˜ä¼˜åŒ– - Axè‡ªåŠ¨é€‰æ‹©å¸•ç´¯æ‰˜å‰æ²¿ä¸Šçš„ä¸€ä¸ªè§£ï¼‰
    best_parameters, best_metrics = optimizer.get_best_parameters()
    print(f"\nå¸•ç´¯æ‰˜æœ€ä¼˜è§£ï¼ˆAxè‡ªåŠ¨é€‰æ‹©ï¼‰ï¼š")
    print("å‚æ•°é…ç½®:")
    for param_name, value in best_parameters.items():
        print(f"  {param_name}: {value}")
    print("æŒ‡æ ‡å€¼:")
    for metric_name, (mean, sem) in best_metrics.items():
        print(f"  {metric_name}: {mean:.4f} Â± {sem:.4f}")
    

    

    
    # è·å–ä¼˜åŒ–å†å²
    history = optimizer.get_optimization_history()
    print("\nä¼˜åŒ–å†å²ï¼š")
    print(history.to_string(index=False))
    
    print("\n" + "="*50)
    print("æµ‹è¯•è‡ªå®šä¹‰ä»£ç†æ¨¡å‹å’Œæ ¸å‡½æ•°é…ç½®")
    print("="*50)
    
    # åˆ›å»ºä½¿ç”¨ MatÃ©rn-5/2 æ ¸å‡½æ•°çš„ä¼˜åŒ–å™¨
    # æ³¨æ„ï¼šå³ä½¿æ˜¯å¤šç›®æ ‡ä¼˜åŒ–ï¼ŒAxé€šå¸¸ä¸ºæ¯ä¸ªç›®æ ‡ä½¿ç”¨ç‹¬ç«‹çš„SingleTaskGPæ¨¡å‹
    custom_optimizer = BayesianOptimizer(
        search_space=search_space,
        optimization_config=optimization_config,
        experiment_name="custom_kernel_optimization",
        random_seed=42,
        # ä½¿ç”¨è‡ªå®šä¹‰é…ç½® - æ¯ä¸ªç›®æ ‡ä¼šä½¿ç”¨ç‹¬ç«‹çš„SingleTaskGP + MatÃ©rnæ ¸
        surrogate_model_class=SingleTaskGP,  # å¤šç›®æ ‡æ—¶Axä¸ºæ¯ä¸ªç›®æ ‡åˆ›å»ºç‹¬ç«‹çš„SingleTaskGP
        kernel_class=MaternKernel,           # ä½¿ç”¨ MatÃ©rn æ ¸å‡½æ•°
        kernel_options={"nu": 2.5}          # MatÃ©rn-5/2 æ ¸å‡½æ•°
    )
    
    print("\n" + "="*50)
    print("æµ‹è¯•è‡ªå®šä¹‰é‡‡é›†å‡½æ•°é…ç½®")
    print("="*50)
    
    # åˆ›å»ºä½¿ç”¨ qUpperConfidenceBound é‡‡é›†å‡½æ•°çš„ä¼˜åŒ–å™¨
    acquisition_optimizer = BayesianOptimizer(
        search_space=search_space,
        optimization_config=optimization_config,
        experiment_name="custom_acquisition_optimization",
        random_seed=42,
        # ä½¿ç”¨è‡ªå®šä¹‰é‡‡é›†å‡½æ•°
        acquisition_function_class=qExpectedHypervolumeImprovement,  # å¤šç›®æ ‡ä¼˜åŒ–ä½¿ç”¨EHVI
        acquisition_function_options={}  # å¯ä»¥æ·»åŠ é‡‡é›†å‡½æ•°å‚æ•°
    )
    
    # ä¸ºè‡ªå®šä¹‰ä¼˜åŒ–å™¨ä¹Ÿæ·»åŠ ä¸€äº›å…ˆéªŒæ•°æ®ï¼Œé¿å…å˜æ¢é”™è¯¯
    custom_optimizer.add_prior_experiments(prior_experiments)
    
    # è¿è¡Œå‡ æ¬¡è¯•éªŒ
    next_trials = custom_optimizer.get_next_parameters(n=2)
    for i, (parameters, trial_index) in enumerate(next_trials):
        print(f"\nè‡ªå®šä¹‰é…ç½® - ç¬¬ {i+1} æ¬¡å®éªŒå‚æ•°ï¼š")
        print(parameters)
        
        # æ¨¡æ‹Ÿå®éªŒç»“æœ
        roughness = np.random.uniform(1.5, 3.0)
        kerf_width = np.random.uniform(0.1, 0.2)
        
        custom_optimizer.update_experiment(
            trial_index=trial_index,
            metrics={"roughness": roughness, "kerf_width": kerf_width}
        )
    
    print("\nè‡ªå®šä¹‰é…ç½®ä¼˜åŒ–å™¨åˆ›å»ºæˆåŠŸï¼")
    print("é…ç½®è¯¦æƒ…:")
    print("- å¤šç›®æ ‡ä¼˜åŒ–ï¼šroughness + kerf_width")
    print("- ä»£ç†æ¨¡å‹ï¼šSingleTaskGPï¼ˆæ¯ä¸ªç›®æ ‡ç‹¬ç«‹å»ºæ¨¡ï¼‰")
    print("- æ ¸å‡½æ•°ï¼šMatÃ©rn-5/2 (nu=2.5)")
    print("- å¯¹æ¯”é»˜è®¤é…ç½®ï¼šé»˜è®¤ä½¿ç”¨RBFæ ¸å‡½æ•°")
    
    # ä¸ºé‡‡é›†å‡½æ•°ä¼˜åŒ–å™¨ä¹Ÿæ·»åŠ ä¸€äº›å…ˆéªŒæ•°æ®ï¼Œé¿å…å˜æ¢é”™è¯¯
    acquisition_optimizer.add_prior_experiments(prior_experiments)
    
    # è¿è¡Œå‡ æ¬¡è¯•éªŒ
    next_trials = acquisition_optimizer.get_next_parameters(n=2)
    for i, (parameters, trial_index) in enumerate(next_trials):
        print(f"\nè‡ªå®šä¹‰é‡‡é›†å‡½æ•° - ç¬¬ {i+1} æ¬¡å®éªŒå‚æ•°ï¼š")
        print(parameters)
        
        # æ¨¡æ‹Ÿå®éªŒç»“æœ
        roughness = np.random.uniform(1.5, 3.0)
        kerf_width = np.random.uniform(0.1, 0.2)
        
        acquisition_optimizer.update_experiment(
            trial_index=trial_index,
            metrics={"roughness": roughness, "kerf_width": kerf_width}
        )
    
    print("\nè‡ªå®šä¹‰é‡‡é›†å‡½æ•°ä¼˜åŒ–å™¨åˆ›å»ºæˆåŠŸï¼")
    print("é…ç½®è¯¦æƒ…:")
    print("- å¤šç›®æ ‡ä¼˜åŒ–ï¼šroughness + kerf_width")
    print("- é‡‡é›†å‡½æ•°ï¼šqExpectedHypervolumeImprovement (EHVI)")
    print("- ä¼˜åŠ¿ï¼šé€‚åˆå¤šç›®æ ‡ä¼˜åŒ–ï¼Œç›´æ¥ä¼˜åŒ–è¶…ä½“ç§¯æŒ‡æ ‡")
    print("- å¯¹æ¯”é»˜è®¤é…ç½®ï¼šé»˜è®¤ä½¿ç”¨qNoisyExpectedImprovement")
    
    print("\n" + "="*60)
    print("ç»¼åˆæµ‹è¯•ï¼šåŒæ—¶ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹ã€æ ¸å‡½æ•°å’Œé‡‡é›†å‡½æ•°")
    print("="*60)
    
    # åˆ›å»ºä¸€ä¸ªåŒæ—¶ä½¿ç”¨æ‰€æœ‰è‡ªå®šä¹‰é…ç½®çš„ä¼˜åŒ–å™¨
    comprehensive_optimizer = BayesianOptimizer(
        search_space=search_space,
        optimization_config=optimization_config,
        experiment_name="comprehensive_custom_optimization",
        random_seed=42,
        # åŒæ—¶ä½¿ç”¨æ‰€æœ‰è‡ªå®šä¹‰é…ç½®
        surrogate_model_class=SingleTaskGP,
        kernel_class=MaternKernel,
        kernel_options={"nu": 1.5},  # MatÃ©rn-3/2 æ ¸
        acquisition_function_class=qExpectedHypervolumeImprovement,
        acquisition_function_options={}  # å¯æ ¹æ®éœ€è¦æ·»åŠ å‚æ•°
    )
    
    print("ç»¼åˆè‡ªå®šä¹‰ä¼˜åŒ–å™¨åˆ›å»ºæˆåŠŸï¼")
    print("æœ€ç»ˆé…ç½®:")
    print("- ä»£ç†æ¨¡å‹: SingleTaskGP")
    print("- æ ¸å‡½æ•°: MatÃ©rn-3/2 (nu=1.5)")
    print("- é‡‡é›†å‡½æ•°: qExpectedHypervolumeImprovement")
    print("- ä¼˜åŠ¿: ç»¼åˆäº†æ‰€æœ‰è‡ªå®šä¹‰é…ç½®ï¼Œå¯å®ç°é«˜åº¦ä¸ªæ€§åŒ–çš„ä¼˜åŒ–")

def get_available_models_kernels_and_acquisitions():
    """
    è¿”å›æ‰€æœ‰å¯é…ç½®çš„ä»£ç†æ¨¡å‹ã€æ ¸å‡½æ•°å’Œé‡‡é›†å‡½æ•°çš„è¯¦ç»†ä¿¡æ¯
    
    Returns:
        dict: åŒ…å«ä»£ç†æ¨¡å‹ã€æ ¸å‡½æ•°å’Œé‡‡é›†å‡½æ•°é…ç½®é€‰é¡¹çš„å­—å…¸
    """
    
    models_info = {
        "ä»£ç†æ¨¡å‹ (Surrogate Models)": {
            "SingleTaskGP": {
                "æè¿°": "æ ‡å‡†å•ä»»åŠ¡é«˜æ–¯è¿‡ç¨‹ï¼Œé€‚ç”¨äºå¤§å¤šæ•°ä¼˜åŒ–é—®é¢˜",
                "é€‚ç”¨åœºæ™¯": "å•ç›®æ ‡æˆ–å¤šç›®æ ‡ç‹¬ç«‹å»ºæ¨¡",
                "ç‰¹ç‚¹": "ç®€å•å¯é ï¼ŒAxé»˜è®¤æ¨è"
            },
            "MultiTaskGP": {
                "æè¿°": "å¤šä»»åŠ¡é«˜æ–¯è¿‡ç¨‹ï¼Œèƒ½åˆ©ç”¨ä»»åŠ¡é—´ç›¸å…³æ€§",
                "é€‚ç”¨åœºæ™¯": "å¤šä¸ªç›¸å…³ä»»åŠ¡ï¼Œéœ€è¦ä»»åŠ¡ç‰¹å¾",
                "ç‰¹ç‚¹": "å¯ä»¥å…±äº«ä¿¡æ¯ï¼Œæé«˜æ•°æ®æ•ˆç‡"
            },
            "KroneckerMultiTaskGP": {
                "æè¿°": "Kroneckerç»“æ„çš„å¤šä»»åŠ¡GPï¼Œé€‚ç”¨äºç»“æ„åŒ–å¤šä»»åŠ¡",
                "é€‚ç”¨åœºæ™¯": "ä»»åŠ¡å…·æœ‰Kroneckerç§¯ç»“æ„",
                "ç‰¹ç‚¹": "è®¡ç®—æ•ˆç‡é«˜ï¼Œé€‚åˆå¤§è§„æ¨¡å¤šä»»åŠ¡"
            },
            "MixedSingleTaskGP": {
                "æè¿°": "æ”¯æŒæ··åˆå˜é‡ç±»å‹ï¼ˆè¿ç»­+ç¦»æ•£ï¼‰çš„GP",
                "é€‚ç”¨åœºæ™¯": "åŒæ—¶åŒ…å«è¿ç»­å’Œåˆ†ç±»å˜é‡çš„ä¼˜åŒ–",
                "ç‰¹ç‚¹": "å¤„ç†æ··åˆå˜é‡ç±»å‹"
            },
            "SingleTaskMultiFidelityGP": {
                "æè¿°": "å¤šä¿çœŸåº¦å•ä»»åŠ¡GPï¼Œæ”¯æŒä¸åŒç²¾åº¦çš„è¯„ä¼°",
                "é€‚ç”¨åœºæ™¯": "æœ‰å¤šä¸ªè¯„ä¼°ç²¾åº¦çº§åˆ«",
                "ç‰¹ç‚¹": "å¯ä»¥åˆ©ç”¨ä½æˆæœ¬çš„è¿‘ä¼¼è¯„ä¼°"
            },
            "SaasFullyBayesianSingleTaskGP": {
                "æè¿°": "å…¨è´å¶æ–¯å•ä»»åŠ¡GPï¼Œä½¿ç”¨Spike-and-Slabå…ˆéªŒ",
                "é€‚ç”¨åœºæ™¯": "é«˜ç»´é—®é¢˜ï¼Œéœ€è¦ç‰¹å¾é€‰æ‹©",
                "ç‰¹ç‚¹": "è‡ªåŠ¨ç‰¹å¾é€‰æ‹©ï¼Œé€‚åˆé«˜ç»´ç¨€ç–é—®é¢˜"
            },
            "SaasFullyBayesianMultiTaskGP": {
                "æè¿°": "å…¨è´å¶æ–¯å¤šä»»åŠ¡GPç‰ˆæœ¬",
                "é€‚ç”¨åœºæ™¯": "é«˜ç»´å¤šä»»åŠ¡é—®é¢˜",
                "ç‰¹ç‚¹": "ç»“åˆå¤šä»»åŠ¡å­¦ä¹ å’Œç‰¹å¾é€‰æ‹©"
            },
            "HigherOrderGP": {
                "æè¿°": "é«˜é˜¶é«˜æ–¯è¿‡ç¨‹ï¼Œæ•æ‰å¤æ‚çš„é«˜é˜¶ç›¸äº’ä½œç”¨",
                "é€‚ç”¨åœºæ™¯": "å­˜åœ¨å¤æ‚å˜é‡äº¤äº’çš„é—®é¢˜",
                "ç‰¹ç‚¹": "èƒ½å»ºæ¨¡é«˜é˜¶äº¤äº’æ•ˆåº”"
            },
            "SingleTaskVariationalGP": {
                "æè¿°": "å˜åˆ†æ¨æ–­çš„GPï¼Œé€‚ç”¨äºå¤§æ•°æ®é›†",
                "é€‚ç”¨åœºæ™¯": "å¤§è§„æ¨¡æ•°æ®é›†ä¼˜åŒ–",
                "ç‰¹ç‚¹": "è®¡ç®—æ•ˆç‡é«˜ï¼Œå¯æ‰©å±•æ€§å¥½"
            }
        },
        
        "æ ¸å‡½æ•° (Kernels)": {
            "RBFKernel": {
                "æè¿°": "å¾„å‘åŸºå‡½æ•°æ ¸ï¼ˆé«˜æ–¯æ ¸ï¼‰ï¼Œå¹³æ»‘ä¸”æ— é™å¯å¾®",
                "å‚æ•°": "lengthscale (é•¿åº¦å°ºåº¦)",
                "é€‚ç”¨": "å…‰æ»‘å‡½æ•°ï¼Œå¤§å¤šæ•°å·¥ç¨‹é—®é¢˜",
                "ç¤ºä¾‹": "kernel_options={'lengthscale': 1.0}"
            },
            "MaternKernel": {
                "æè¿°": "MatÃ©rnæ ¸ï¼Œé€šè¿‡nuå‚æ•°æ§åˆ¶å¹³æ»‘åº¦",
                "å‚æ•°": "nu (0.5, 1.5, 2.5, æ— ç©·å¤§), lengthscale",
                "é€‚ç”¨": "ä¸åŒå¹³æ»‘åº¦éœ€æ±‚ï¼Œå·¥ç¨‹ä¼˜åŒ–å¸¸ç”¨",
                "ç¤ºä¾‹": "kernel_options={'nu': 2.5, 'lengthscale': 1.0}"
            },
            "LinearKernel": {
                "æè¿°": "çº¿æ€§æ ¸ï¼Œå»ºæ¨¡çº¿æ€§å…³ç³»",
                "å‚æ•°": "variance",
                "é€‚ç”¨": "çº¿æ€§æˆ–è¿‘ä¼¼çº¿æ€§é—®é¢˜",
                "ç¤ºä¾‹": "kernel_options={'variance': 1.0}"
            },
            "PolynomialKernel": {
                "æè¿°": "å¤šé¡¹å¼æ ¸ï¼Œå»ºæ¨¡å¤šé¡¹å¼å…³ç³»",
                "å‚æ•°": "power (å¹‚æ¬¡), offset",
                "é€‚ç”¨": "å¤šé¡¹å¼å…³ç³»çš„é—®é¢˜",
                "ç¤ºä¾‹": "kernel_options={'power': 2, 'offset': 1.0}"
            },
            "PeriodicKernel": {
                "æè¿°": "å‘¨æœŸæ ¸ï¼Œå»ºæ¨¡å‘¨æœŸæ€§æ¨¡å¼",
                "å‚æ•°": "period (å‘¨æœŸ), lengthscale",
                "é€‚ç”¨": "å…·æœ‰å‘¨æœŸæ€§çš„ä¼˜åŒ–é—®é¢˜",
                "ç¤ºä¾‹": "kernel_options={'period': 1.0, 'lengthscale': 1.0}"
            },
            "SpectralMixtureKernel": {
                "æè¿°": "è°±æ··åˆæ ¸ï¼Œå¯ä»¥è¿‘ä¼¼ä»»æ„å¹³ç¨³æ ¸",
                "å‚æ•°": "num_mixtures (æ··åˆæ•°é‡)",
                "é€‚ç”¨": "å¤æ‚çš„é¢‘åŸŸç‰¹å¾",
                "ç¤ºä¾‹": "kernel_options={'num_mixtures': 4}"
            },
            "RQKernel": {
                "æè¿°": "æœ‰ç†äºŒæ¬¡æ ¸ï¼Œç»“åˆRBFå’Œå¤šé¡¹å¼ç‰¹æ€§",
                "å‚æ•°": "alpha (å½¢çŠ¶å‚æ•°), lengthscale",
                "é€‚ç”¨": "ä¸­ç­‰å¤æ‚åº¦çš„å¹³æ»‘å‡½æ•°",
                "ç¤ºä¾‹": "kernel_options={'alpha': 2.0, 'lengthscale': 1.0}"
            },
            "CosineKernel": {
                "æè¿°": "ä½™å¼¦æ ¸ï¼Œå»ºæ¨¡ä½™å¼¦ç›¸ä¼¼æ€§",
                "å‚æ•°": "period",
                "é€‚ç”¨": "ä½™å¼¦å‹å‘¨æœŸæ¨¡å¼",
                "ç¤ºä¾‹": "kernel_options={'period': 1.0}"
            },
            "ScaleKernel": {
                "æè¿°": "ç¼©æ”¾æ ¸ï¼Œä¸ºå…¶ä»–æ ¸æ·»åŠ è¾“å‡ºç¼©æ”¾",
                "å‚æ•°": "base_kernel, outputscale",
                "é€‚ç”¨": "éœ€è¦è°ƒæ•´è¾“å‡ºå°ºåº¦çš„æƒ…å†µ",
                "ç¤ºä¾‹": "ç”¨ä½œåŒ…è£…å™¨æ ¸"
            },
            "AdditiveKernel": {
                "æè¿°": "åŠ æ€§æ ¸ï¼Œç»„åˆå¤šä¸ªæ ¸å‡½æ•°",
                "å‚æ•°": "kern1, kern2 (è¦ç»„åˆçš„æ ¸)",
                "é€‚ç”¨": "éœ€è¦ç»„åˆä¸åŒç±»å‹ç›¸å…³æ€§",
                "ç¤ºä¾‹": "RBF + Linear ç»„åˆ"
            },
            "ProductKernel": {
                "æè¿°": "ä¹˜ç§¯æ ¸ï¼Œæ ¸å‡½æ•°çš„ä¹˜ç§¯",
                "å‚æ•°": "kern1, kern2 (è¦ç›¸ä¹˜çš„æ ¸)",
                "é€‚ç”¨": "éœ€è¦æ ¸å‡½æ•°ä¹˜ç§¯çš„åœºæ™¯",
                "ç¤ºä¾‹": "Periodic Ã— RBF"
            }
        },
        
        "é‡‡é›†å‡½æ•° (Acquisition Functions)": {
            "qExpectedImprovement": {
                "æè¿°": "æœŸæœ›æ”¹è¿›ï¼ˆæ‰¹é‡ç‰ˆæœ¬ï¼‰ï¼Œå¹³è¡¡å¼€å‘ä¸æ¢ç´¢",
                "é€‚ç”¨": "å•ç›®æ ‡ä¼˜åŒ–ï¼Œå‡è¡¡çš„æ¢ç´¢-å¼€å‘ç­–ç•¥",
                "å‚æ•°": "eta (çº¦æŸå¹³æ»‘åº¦ï¼Œé»˜è®¤1e-3)",
                "ç¤ºä¾‹": "acquisition_function_options={'eta': 1e-3}"
            },
            "qNoisyExpectedImprovement": {
                "æè¿°": "å™ªå£°æœŸæœ›æ”¹è¿›ï¼Œè€ƒè™‘è§‚æµ‹å™ªå£°çš„EI",
                "é€‚ç”¨": "å•ç›®æ ‡ä¼˜åŒ–ï¼Œå­˜åœ¨è§‚æµ‹å™ªå£°",
                "å‚æ•°": "eta (çº¦æŸå¹³æ»‘åº¦ï¼Œé»˜è®¤1e-3)",
                "ç¤ºä¾‹": "acquisition_function_options={'eta': 1e-3}"
            },
            "qUpperConfidenceBound": {
                "æè¿°": "ä¸Šç½®ä¿¡ç•Œï¼Œå¯è°ƒèŠ‚æ¢ç´¢ç¨‹åº¦",
                "é€‚ç”¨": "å•ç›®æ ‡ä¼˜åŒ–ï¼Œéœ€è¦æ§åˆ¶æ¢ç´¢-å¼€å‘å¹³è¡¡",
                "å‚æ•°": "beta (æ¢ç´¢æƒé‡ï¼Œé»˜è®¤0.2)",
                "ç¤ºä¾‹": "acquisition_function_options={'beta': 0.1}"
            },
            "qKnowledgeGradient": {
                "æè¿°": "çŸ¥è¯†æ¢¯åº¦ï¼Œè€ƒè™‘ä¿¡æ¯ä»·å€¼",
                "é€‚ç”¨": "å•ç›®æ ‡ä¼˜åŒ–ï¼Œé‡è§†ä¿¡æ¯è·å–",
                "å‚æ•°": "num_fantasies (å¹»æƒ³æ ·æœ¬æ•°)",
                "ç¤ºä¾‹": "acquisition_function_options={'num_fantasies': 128}"
            },
            "qLogExpectedImprovement": {
                "æè¿°": "å¯¹æ•°æœŸæœ›æ”¹è¿›ï¼Œæ•°å€¼ç¨³å®šæ€§æ›´å¥½",
                "é€‚ç”¨": "å•ç›®æ ‡ä¼˜åŒ–ï¼Œæ”¹è¿›å€¼è¾ƒå°æ—¶",
                "å‚æ•°": "æ— ç‰¹æ®Šå‚æ•°",
                "ç¤ºä¾‹": "acquisition_function_class=qLogExpectedImprovement"
            },
            "qMaxValueEntropy": {
                "æè¿°": "æœ€å¤§å€¼ç†µæœç´¢ï¼Œä¼˜åŒ–æœ€å¤§å€¼çš„ä¸ç¡®å®šæ€§",
                "é€‚ç”¨": "å•ç›®æ ‡ä¼˜åŒ–ï¼Œé«˜æ•ˆçš„å…¨å±€æœç´¢",
                "å‚æ•°": "num_mv_samples (æœ€å¤§å€¼æ ·æœ¬æ•°)",
                "ç¤ºä¾‹": "acquisition_function_options={'num_mv_samples': 10}"
            },
            "qExpectedHypervolumeImprovement": {
                "æè¿°": "æœŸæœ›è¶…ä½“ç§¯æ”¹è¿›ï¼Œå¤šç›®æ ‡ä¼˜åŒ–ç»å…¸æ–¹æ³•",
                "é€‚ç”¨": "å¤šç›®æ ‡ä¼˜åŒ–ï¼Œç›´æ¥ä¼˜åŒ–å¸•ç´¯æ‰˜å‰æ²¿",
                "å‚æ•°": "ref_point (å‚è€ƒç‚¹ï¼Œå¯é€‰)",
                "ç¤ºä¾‹": "acquisition_function_class=qExpectedHypervolumeImprovement"
            },
            "qNoisyExpectedHypervolumeImprovement": {
                "æè¿°": "å™ªå£°æœŸæœ›è¶…ä½“ç§¯æ”¹è¿›",
                "é€‚ç”¨": "å¤šç›®æ ‡ä¼˜åŒ–ï¼Œå­˜åœ¨è§‚æµ‹å™ªå£°",
                "å‚æ•°": "ref_point (å‚è€ƒç‚¹ï¼Œå¯é€‰)",
                "ç¤ºä¾‹": "acquisition_function_class=qNoisyExpectedHypervolumeImprovement"
            },
            "qLogExpectedHypervolumeImprovement": {
                "æè¿°": "å¯¹æ•°æœŸæœ›è¶…ä½“ç§¯æ”¹è¿›ï¼Œæ•°å€¼ç¨³å®š",
                "é€‚ç”¨": "å¤šç›®æ ‡ä¼˜åŒ–ï¼Œæ”¹è¿›å€¼è¾ƒå°æ—¶",
                "å‚æ•°": "ref_point (å‚è€ƒç‚¹ï¼Œå¯é€‰)",
                "ç¤ºä¾‹": "acquisition_function_class=qLogExpectedHypervolumeImprovement"
            },
            "qLogNParEGO": {
                "æè¿°": "ParEGOçš„å¯¹æ•°ç‰ˆæœ¬ï¼Œå°†å¤šç›®æ ‡è½¬ä¸ºå•ç›®æ ‡",
                "é€‚ç”¨": "å¤šç›®æ ‡ä¼˜åŒ–ï¼Œè®¡ç®—èµ„æºæœ‰é™æ—¶",
                "å‚æ•°": "æ— ç‰¹æ®Šå‚æ•°",
                "ç¤ºä¾‹": "acquisition_function_class=qLogNParEGO"
            },
            "PosteriorMean": {
                "æè¿°": "åéªŒå‡å€¼ï¼Œçº¯å¼€å‘ç­–ç•¥",
                "é€‚ç”¨": "å•ç›®æ ‡ä¼˜åŒ–ï¼Œå·²çŸ¥æœ€ä¼˜åŒºåŸŸï¼Œç²¾ç¡®æœç´¢",
                "å‚æ•°": "æ— ç‰¹æ®Šå‚æ•°",
                "ç¤ºä¾‹": "acquisition_function_class=PosteriorMean"
            },
            "ExpectedImprovement": {
                "æè¿°": "ç»å…¸æœŸæœ›æ”¹è¿›ï¼ˆè§£æç‰ˆæœ¬ï¼‰",
                "é€‚ç”¨": "å•ç›®æ ‡ä¼˜åŒ–ï¼Œè®¡ç®—é«˜æ•ˆ",
                "å‚æ•°": "æ— ç‰¹æ®Šå‚æ•°", 
                "ç¤ºä¾‹": "acquisition_function_class=ExpectedImprovement"
            },
            "UpperConfidenceBound": {
                "æè¿°": "ç»å…¸ä¸Šç½®ä¿¡ç•Œï¼ˆè§£æç‰ˆæœ¬ï¼‰",
                "é€‚ç”¨": "å•ç›®æ ‡ä¼˜åŒ–ï¼Œè®¡ç®—é«˜æ•ˆ",
                "å‚æ•°": "beta (æ¢ç´¢æƒé‡)",
                "ç¤ºä¾‹": "acquisition_function_options={'beta': 0.1}"
            }
        }
    }
    
    return models_info

def print_configuration_guide():
    """æ‰“å°ä»£ç†æ¨¡å‹ã€æ ¸å‡½æ•°å’Œé‡‡é›†å‡½æ•°çš„é…ç½®æŒ‡å—"""
    info = get_available_models_kernels_and_acquisitions()
    
    print("ğŸ”§ Axä¼˜åŒ–å™¨ - ä»£ç†æ¨¡å‹ã€æ ¸å‡½æ•°å’Œé‡‡é›†å‡½æ•°é…ç½®æŒ‡å—")
    print("=" * 70)
    
    for category, items in info.items():
        print(f"\nğŸ“‹ {category}")
        print("-" * 40)
        
        for name, details in items.items():
            print(f"\nğŸ”¸ {name}")
            for key, value in details.items():
                print(f"   {key}: {value}")
    
    print("\n" + "=" * 70)
    print("ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹:")
    print("optimizer = BayesianOptimizer(")
    print("    search_space=search_space,")
    print("    optimization_config=optimization_config,")
    print("    surrogate_model_class=SingleTaskGP,              # é€‰æ‹©ä»£ç†æ¨¡å‹")
    print("    kernel_class=MaternKernel,                       # é€‰æ‹©æ ¸å‡½æ•°")
    print("    kernel_options={'nu': 2.5},                     # æ ¸å‡½æ•°å‚æ•°")
    print("    acquisition_function_class=qExpectedImprovement, # é€‰æ‹©é‡‡é›†å‡½æ•°")
    print("    acquisition_function_options={}                 # é‡‡é›†å‡½æ•°å‚æ•°")
    print(")")
    
    print("\nğŸ¯ å¸¸ç”¨ç»„åˆæ¨è:")
    print("1. å•ç›®æ ‡é€šç”¨ä¼˜åŒ–: SingleTaskGP + MaternKernel(nu=2.5) + qExpectedImprovement")
    print("2. å•ç›®æ ‡å™ªå£°ç¯å¢ƒ: SingleTaskGP + RBFKernel + qNoisyExpectedImprovement")
    print("3. å•ç›®æ ‡æ¢ç´¢é‡ç‚¹: SingleTaskGP + MaternKernel + qUpperConfidenceBound(beta=0.1)")
    print("4. å¤šç›®æ ‡ä¼˜åŒ–: SingleTaskGP + MaternKernel + qExpectedHypervolumeImprovement")
    print("5. å¤šç›®æ ‡å™ªå£°: SingleTaskGP + RBFKernel + qNoisyExpectedHypervolumeImprovement")
    print("6. é«˜ç»´ç¨€ç–: SaasFullyBayesianSingleTaskGP + MaternKernel + qLogExpectedImprovement")
    print("7. å¿«é€Ÿæ”¶æ•›: SingleTaskGP + RBFKernel + qKnowledgeGradient")
    print("8. å…¨å±€æœç´¢: SingleTaskGP + MaternKernel + qMaxValueEntropy")

def test_single_objective_acquisition_functions():
    """
    æµ‹è¯•å•ç›®æ ‡ä¼˜åŒ–çš„ä¸åŒé‡‡é›†å‡½æ•°
    """
    print("ğŸ¯ å•ç›®æ ‡ä¼˜åŒ– - é‡‡é›†å‡½æ•°å¯¹æ¯”æµ‹è¯•")
    print("=" * 60)
    
    # å®šä¹‰å•ç›®æ ‡æœç´¢ç©ºé—´ï¼ˆæ¿€å…‰åˆ‡å‰²ä¼˜åŒ–ï¼‰
    search_space = [
        {
            "name": "power",
            "type": "range", 
            "bounds": [1000, 3000],
            "value_type": "int"
        },
        {
            "name": "speed",
            "type": "range",
            "bounds": [10, 50], 
            "value_type": "float"
        },
        {
            "name": "frequency",
            "type": "choice",
            "values": [500, 1000, 1500, 2000],
            "value_type": "int"
        }
    ]
    
    # å•ç›®æ ‡ä¼˜åŒ–é…ç½® - åªä¼˜åŒ–è¡¨é¢ç²—ç³™åº¦
    optimization_config = {
        "objectives": {
            "roughness": {"minimize": True}
        },
        "additional_metrics": ["kerf_width"]  # ä½œä¸ºé¢å¤–è·Ÿè¸ªæŒ‡æ ‡
    }
    
    # å…ˆéªŒå®éªŒæ•°æ®
    prior_experiments = [
        ExperimentResult(
            parameters={"power": 2000, "speed": 30.0, "frequency": 1000},
            metrics={"roughness": 2.5, "kerf_width": 0.15}
        ),
        ExperimentResult(
            parameters={"power": 2500, "speed": 40.0, "frequency": 1500},
            metrics={"roughness": 1.8, "kerf_width": 0.18}
        )
    ]
    
    # æµ‹è¯•ä¸åŒçš„é‡‡é›†å‡½æ•°
    acquisition_functions = [
        {
            "name": "qExpectedImprovement",
            "class": qExpectedImprovement,
            "options": {},
            "description": "æœŸæœ›æ”¹è¿› - å‡è¡¡æ¢ç´¢ä¸å¼€å‘"
        },
        {
            "name": "qNoisyExpectedImprovement", 
            "class": qNoisyExpectedImprovement,
            "options": {},
            "description": "å™ªå£°æœŸæœ›æ”¹è¿› - è€ƒè™‘è§‚æµ‹å™ªå£°"
        },
        {
            "name": "qUpperConfidenceBound",
            "class": qUpperConfidenceBound,
            "options": {"beta": 0.1},
            "description": "ä¸Šç½®ä¿¡ç•Œ - æ¢ç´¢å¯¼å‘(beta=0.1)"
        },
        {
            "name": "qLogExpectedImprovement",
            "class": qLogExpectedImprovement, 
            "options": {},
            "description": "å¯¹æ•°æœŸæœ›æ”¹è¿› - æ•°å€¼ç¨³å®š"
        }
    ]
    
    for i, acq_func in enumerate(acquisition_functions, 1):
        print(f"\nğŸ“‹ æµ‹è¯• {i}: {acq_func['name']}")
        print(f"æè¿°: {acq_func['description']}")
        print("-" * 50)
        
        try:
            # åˆ›å»ºä¼˜åŒ–å™¨
            optimizer = BayesianOptimizer(
                search_space=search_space,
                optimization_config=optimization_config,
                experiment_name=f"single_obj_{acq_func['name']}",
                random_seed=42,
                surrogate_model_class=SingleTaskGP,
                kernel_class=MaternKernel,
                kernel_options={"nu": 2.5},
                acquisition_function_class=acq_func['class'],
                acquisition_function_options=acq_func['options']
            )
            
            # æ·»åŠ å…ˆéªŒæ•°æ®
            optimizer.add_prior_experiments(prior_experiments)
            
            # è·å–ä¸‹ä¸€ç»„å‚æ•°
            next_trials = optimizer.get_next_parameters(n=2)
            
            for j, (parameters, trial_index) in enumerate(next_trials, 1):
                print(f"  è¯•éªŒ {j}: {parameters}")
                
                # æ¨¡æ‹Ÿå®éªŒç»“æœ
                roughness = np.random.uniform(1.5, 3.0)
                kerf_width = np.random.uniform(0.1, 0.2)
                
                optimizer.update_experiment(
                    trial_index=trial_index,
                    metrics={"roughness": roughness, "kerf_width": kerf_width}
                )
            
            # è·å–æœ€ä¼˜ç»“æœ
            best_params, best_metrics = optimizer.get_best_parameters()
            print(f"  æœ€ä¼˜ç²—ç³™åº¦: {best_metrics.get('roughness', (0, 0))[0]:.4f}")
            print(f"  âœ… {acq_func['name']} æµ‹è¯•æˆåŠŸ")
            
        except Exception as e:
            print(f"  âŒ {acq_func['name']} æµ‹è¯•å¤±è´¥: {e}")

def test_multi_objective_acquisition_functions():
    """
    æµ‹è¯•å¤šç›®æ ‡ä¼˜åŒ–çš„ä¸åŒé‡‡é›†å‡½æ•°
    """
    print("\nğŸ¯ å¤šç›®æ ‡ä¼˜åŒ– - é‡‡é›†å‡½æ•°å¯¹æ¯”æµ‹è¯•")
    print("=" * 60)
    
    # å®šä¹‰æœç´¢ç©ºé—´
    search_space = [
        {
            "name": "power",
            "type": "range",
            "bounds": [1000, 3000], 
            "value_type": "int"
        },
        {
            "name": "speed",
            "type": "range",
            "bounds": [10, 50],
            "value_type": "float"
        },
        {
            "name": "frequency",
            "type": "choice",
            "values": [500, 1000, 1500, 2000],
            "value_type": "int"
        }
    ]
    
    # å¤šç›®æ ‡ä¼˜åŒ–é…ç½®
    optimization_config = {
        "objectives": {
            "roughness": {"minimize": True},
            "kerf_width": {"minimize": True}
        }
    }
    
    # å…ˆéªŒæ•°æ®
    prior_experiments = [
        ExperimentResult(
            parameters={"power": 2000, "speed": 30.0, "frequency": 1000},
            metrics={"roughness": 2.5, "kerf_width": 0.15}
        ),
        ExperimentResult(
            parameters={"power": 2500, "speed": 40.0, "frequency": 1500},
            metrics={"roughness": 1.8, "kerf_width": 0.18}
        )
    ]
    
    # æµ‹è¯•å¤šç›®æ ‡é‡‡é›†å‡½æ•°
    multi_obj_acquisition_functions = [
        {
            "name": "qExpectedHypervolumeImprovement",
            "class": qExpectedHypervolumeImprovement,
            "options": {},
            "description": "æœŸæœ›è¶…ä½“ç§¯æ”¹è¿› - ç»å…¸å¤šç›®æ ‡æ–¹æ³•"
        },
        {
            "name": "qNoisyExpectedHypervolumeImprovement",
            "class": qNoisyExpectedHypervolumeImprovement, 
            "options": {},
            "description": "å™ªå£°æœŸæœ›è¶…ä½“ç§¯æ”¹è¿› - è€ƒè™‘å™ªå£°"
        },
        {
            "name": "qLogExpectedHypervolumeImprovement",
            "class": qLogExpectedHypervolumeImprovement,
            "options": {},
            "description": "å¯¹æ•°æœŸæœ›è¶…ä½“ç§¯æ”¹è¿› - æ•°å€¼ç¨³å®š(æ¨è)"
        },
        {
            "name": "qLogNParEGO",
            "class": qLogNParEGO,
            "options": {},
            "description": "ParEGOå¯¹æ•°ç‰ˆæœ¬ - è®¡ç®—é«˜æ•ˆ"
        }
    ]
    
    for i, acq_func in enumerate(multi_obj_acquisition_functions, 1):
        print(f"\nğŸ“‹ æµ‹è¯• {i}: {acq_func['name']}")
        print(f"æè¿°: {acq_func['description']}")
        print("-" * 50)
        
        try:
            # åˆ›å»ºä¼˜åŒ–å™¨
            optimizer = BayesianOptimizer(
                search_space=search_space,
                optimization_config=optimization_config,
                experiment_name=f"multi_obj_{acq_func['name']}",
                random_seed=42,
                surrogate_model_class=SingleTaskGP,
                kernel_class=MaternKernel,
                kernel_options={"nu": 2.5},
                acquisition_function_class=acq_func['class'],
                acquisition_function_options=acq_func['options']
            )
            
            # æ·»åŠ å…ˆéªŒæ•°æ®
            optimizer.add_prior_experiments(prior_experiments)
            
            # è·å–ä¸‹ä¸€ç»„å‚æ•°
            next_trials = optimizer.get_next_parameters(n=2)
            
            for j, (parameters, trial_index) in enumerate(next_trials, 1):
                print(f"  è¯•éªŒ {j}: {parameters}")
                
                # æ¨¡æ‹Ÿå®éªŒç»“æœ
                roughness = np.random.uniform(1.5, 3.0)
                kerf_width = np.random.uniform(0.1, 0.2)
                
                optimizer.update_experiment(
                    trial_index=trial_index,
                    metrics={"roughness": roughness, "kerf_width": kerf_width}
                )
            
            # è·å–æœ€ä¼˜ç»“æœï¼ˆå¸•ç´¯æ‰˜å‰æ²¿ï¼‰
            best_params, best_metrics = optimizer.get_best_parameters()
            if best_metrics:
                roughness_val = best_metrics.get('roughness', (0, 0))[0]
                kerf_val = best_metrics.get('kerf_width', (0, 0))[0]
                print(f"  å¸•ç´¯æ‰˜è§£: roughness={roughness_val:.4f}, kerf_width={kerf_val:.4f}")
            print(f"  âœ… {acq_func['name']} æµ‹è¯•æˆåŠŸ")
            
        except Exception as e:
            print(f"  âŒ {acq_func['name']} æµ‹è¯•å¤±è´¥: {e}")

def test_acquisition_function_parameters():
    """
    æµ‹è¯•é‡‡é›†å‡½æ•°å‚æ•°å¯¹ä¼˜åŒ–è¡Œä¸ºçš„å½±å“
    """
    print("\nğŸ¯ é‡‡é›†å‡½æ•°å‚æ•°è°ƒä¼˜æµ‹è¯•")
    print("=" * 60)
    
    search_space = [
        {
            "name": "x",
            "type": "range",
            "bounds": [-5, 5],
            "value_type": "float"
        },
        {
            "name": "y", 
            "type": "range",
            "bounds": [-5, 5],
            "value_type": "float"
        }
    ]
    
    # ç®€å•çš„å•ç›®æ ‡ä¼˜åŒ–
    optimization_config = {
        "objectives": {
            "objective": {"minimize": True}
        }
    }
    
    # æµ‹è¯•ä¸åŒçš„betaå€¼å¯¹UCBçš„å½±å“
    beta_values = [0.01, 0.1, 0.5, 1.0]
    
    print("\nğŸ”§ æµ‹è¯• qUpperConfidenceBound çš„ beta å‚æ•°:")
    
    for beta in beta_values:
        print(f"\n  Beta = {beta} ({'ä½æ¢ç´¢' if beta < 0.1 else 'é«˜æ¢ç´¢' if beta > 0.5 else 'ä¸­ç­‰æ¢ç´¢'})")
        
        try:
            optimizer = BayesianOptimizer(
                search_space=search_space,
                optimization_config=optimization_config,
                experiment_name=f"ucb_beta_{beta}",
                random_seed=42,
                acquisition_function_class=qUpperConfidenceBound,
                acquisition_function_options={"beta": beta}
            )
            
            # æ·»åŠ ä¸€äº›åˆå§‹æ•°æ®
            initial_data = [
                ExperimentResult(
                    parameters={"x": 0.0, "y": 0.0},
                    metrics={"objective": 1.0}
                )
            ]
            optimizer.add_prior_experiments(initial_data)
            
            # è·å–ä¸‹ä¸€ä¸ªè¯•éªŒç‚¹
            next_trial = optimizer.get_next_parameters(n=1)[0]
            params = next_trial[0]
            
            print(f"    ä¸‹ä¸€ä¸ªè¯•éªŒç‚¹: x={params['x']:.3f}, y={params['y']:.3f}")
            print(f"    âœ… Beta={beta} æµ‹è¯•æˆåŠŸ")
            
        except Exception as e:
            print(f"    âŒ Beta={beta} æµ‹è¯•å¤±è´¥: {e}")
    
    # æµ‹è¯• EI é‡‡é›†å‡½æ•°çš„ eta å‚æ•°
    print("\nğŸ”§ æµ‹è¯• qExpectedImprovement çš„ eta å‚æ•° (çº¦æŸå¹³æ»‘åº¦):")
    
    eta_values = [1e-4, 1e-3, 1e-2, 1e-1]
    
    for eta in eta_values:
        print(f"\n  Eta = {eta} ({'ä½å¹³æ»‘' if eta < 1e-3 else 'é«˜å¹³æ»‘' if eta > 1e-2 else 'ä¸­ç­‰å¹³æ»‘'})")
        
        try:
            optimizer = BayesianOptimizer(
                search_space=search_space,
                optimization_config=optimization_config,
                experiment_name=f"ei_eta_{eta}",
                random_seed=42,
                acquisition_function_class=qExpectedImprovement,
                acquisition_function_options={"eta": eta}
            )
            
            # æ·»åŠ ä¸€äº›åˆå§‹æ•°æ®
            initial_data = [
                ExperimentResult(
                    parameters={"x": 0.0, "y": 0.0},
                    metrics={"objective": 1.0}
                ),
                ExperimentResult(
                    parameters={"x": 1.0, "y": 1.0},
                    metrics={"objective": 0.5}
                )
            ]
            optimizer.add_prior_experiments(initial_data)
            
            # è·å–ä¸‹ä¸€ä¸ªè¯•éªŒç‚¹
            next_trial = optimizer.get_next_parameters(n=1)[0]
            params = next_trial[0]
            
            print(f"    ä¸‹ä¸€ä¸ªè¯•éªŒç‚¹: x={params['x']:.3f}, y={params['y']:.3f}")
            print(f"    âœ… Eta={eta} æµ‹è¯•æˆåŠŸ")
            
        except Exception as e:
            print(f"    âŒ Eta={eta} æµ‹è¯•å¤±è´¥: {e}")
    
    # æµ‹è¯• qKnowledgeGradient çš„ num_fantasies å‚æ•°
    print("\nğŸ”§ æµ‹è¯• qKnowledgeGradient çš„ num_fantasies å‚æ•°:")
    
    fantasies_values = [16, 64, 128, 256]
    
    for num_fantasies in fantasies_values:
        print(f"\n  Num_fantasies = {num_fantasies} ({'ä½é‡‡æ ·' if num_fantasies < 64 else 'é«˜é‡‡æ ·' if num_fantasies > 128 else 'ä¸­ç­‰é‡‡æ ·'})")
        
        try:
            optimizer = BayesianOptimizer(
                search_space=search_space,
                optimization_config=optimization_config,
                experiment_name=f"kg_fantasies_{num_fantasies}",
                random_seed=42,
                acquisition_function_class=qKnowledgeGradient,
                acquisition_function_options={"num_fantasies": num_fantasies}
            )
            
            # æ·»åŠ ä¸€äº›åˆå§‹æ•°æ®
            initial_data = [
                ExperimentResult(
                    parameters={"x": 0.0, "y": 0.0},
                    metrics={"objective": 1.0}
                )
            ]
            optimizer.add_prior_experiments(initial_data)
            
            # è·å–ä¸‹ä¸€ä¸ªè¯•éªŒç‚¹
            next_trial = optimizer.get_next_parameters(n=1)[0]
            params = next_trial[0]
            
            print(f"    ä¸‹ä¸€ä¸ªè¯•éªŒç‚¹: x={params['x']:.3f}, y={params['y']:.3f}")
            print(f"    âœ… Num_fantasies={num_fantasies} æµ‹è¯•æˆåŠŸ")
            
        except Exception as e:
            print(f"    âŒ Num_fantasies={num_fantasies} æµ‹è¯•å¤±è´¥: {e}")

if __name__ == "__main__":
    # æ‰“å°é…ç½®æŒ‡å—
    print_configuration_guide()
    print("\n" + "=" * 70)
    print("ğŸš€ è¿è¡Œé‡‡é›†å‡½æ•°æµ‹è¯•ç”¨ä¾‹:")
    print("=" * 70)
    
    # è¿è¡Œä¸åŒçš„é‡‡é›†å‡½æ•°æµ‹è¯•
    test_single_objective_acquisition_functions()
    test_multi_objective_acquisition_functions() 
    test_acquisition_function_parameters()
    
    print("\n" + "=" * 70)
    print("ğŸ‰ æ‰€æœ‰é‡‡é›†å‡½æ•°æµ‹è¯•å®Œæˆ!")
    print("=" * 70)
    
    # æ³¨é‡Šæ‰åŸæ¥çš„ç»¼åˆæµ‹è¯•
    # print("\n" + "=" * 60)
    # print("ğŸš€ è¿è¡ŒåŸå§‹æµ‹è¯•ç¤ºä¾‹:")
    # print("=" * 60)
    # test_optimizer()
